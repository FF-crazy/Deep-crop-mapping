{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Vision Transformer å†œä½œç‰©åˆ¶å›¾è®­ç»ƒå’Œè¯„ä¼°\n\næœ¬notebookæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Vision Transformeræ¨¡å‹è¿›è¡Œå¤šå…‰è°±æ—¶åºæ•°æ®çš„å†œä½œç‰©åˆ†ç±»ä»»åŠ¡ã€‚\n\n## ğŸ”„ åŒæ¨¡å¼è®¾è®¡\n\n**ğŸ“‹ æœ¬notebookæ”¯æŒä¸¤ç§è¿è¡Œæ¨¡å¼ï¼š**\n\n### ğŸ§ª TESTæ¨¡å¼ \n- **ç›®çš„**: æœ¬åœ°å¿«é€Ÿæµ‹è¯•ä»£ç æ­£ç¡®æ€§\n- **é…ç½®**: è½»é‡çº§æ¨¡å‹ï¼Œå°æ•°æ®é›†ï¼Œå°‘é‡epochs\n- **ç”¨æ—¶**: ~3åˆ†é’Ÿ\n- **é€‚ç”¨**: å¼€å‘è°ƒè¯•ï¼ŒéªŒè¯ä»£ç é€»è¾‘\n- **ç¯å¢ƒ**: æœ¬åœ°CPU/GPUï¼Œä½å†…å­˜è¦æ±‚\n\n### ğŸš€ TRAINæ¨¡å¼\n- **ç›®çš„**: äº‘ç«¯GPUå®Œæ•´è®­ç»ƒè·å¾—æœ€ä½³æ€§èƒ½\n- **é…ç½®**: æ ‡å‡†æ¨¡å‹ï¼Œå®Œæ•´æ•°æ®é›†ï¼Œå®Œæ•´epochs\n- **ç”¨æ—¶**: ~8-16å°æ—¶\n- **é€‚ç”¨**: æ­£å¼è®­ç»ƒï¼Œè·å¾—éƒ¨ç½²æ¨¡å‹\n- **ç¯å¢ƒ**: äº‘ç«¯GPUï¼Œé«˜æ€§èƒ½è¦æ±‚\n\n**ğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š**\n1. æœ¬åœ°å¼€å‘æ—¶ä½¿ç”¨TESTæ¨¡å¼éªŒè¯ä»£ç \n2. ç¡®è®¤æ— è¯¯ååˆ‡æ¢åˆ°TRAINæ¨¡å¼è¿›è¡Œå®Œæ•´è®­ç»ƒ\n3. åœ¨äº‘ç«¯GPUä¸Šè¿è¡ŒTRAINæ¨¡å¼ä»¥è·å¾—æœ€ä½³æ•ˆæœ\n\n---\n\n## ç›®å½•\n1. [è¿è¡Œæ¨¡å¼é…ç½®](#è¿è¡Œæ¨¡å¼é…ç½®)\n2. [ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥](#ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥)\n3. [æ•°æ®åŠ è½½å’Œæ¢ç´¢](#æ•°æ®åŠ è½½å’Œæ¢ç´¢)\n4. [æ¨¡å‹åˆ›å»ºå’Œé…ç½®](#æ¨¡å‹åˆ›å»ºå’Œé…ç½®)\n5. [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)\n6. [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)\n7. [ç»“æœå¯è§†åŒ–](#ç»“æœå¯è§†åŒ–)\n8. [æ³¨æ„åŠ›æœºåˆ¶åˆ†æ](#æ³¨æ„åŠ›æœºåˆ¶åˆ†æ)\n9. [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)\""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ğŸ”§ è¿è¡Œæ¨¡å¼é…ç½®\n# è®¾ç½® RUNNING_MODE æ¥é€‰æ‹©è¿è¡Œæ¨¡å¼\n# - \"TEST\": æœ¬åœ°æµ‹è¯•æ¨¡å¼ï¼Œå¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\n# - \"TRAIN\": äº‘ç«¯è®­ç»ƒæ¨¡å¼ï¼Œå®Œæ•´è®­ç»ƒæµç¨‹\n\nRUNNING_MODE = \"TEST\"  # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œæ¥åˆ‡æ¢æ¨¡å¼: \"TEST\" æˆ– \"TRAIN\"\n\nprint(f\"ğŸ”„ å½“å‰è¿è¡Œæ¨¡å¼: {RUNNING_MODE}\")\n\nif RUNNING_MODE == \"TEST\":\n    print(\"ğŸ§ª TESTæ¨¡å¼ - ç”¨äºæœ¬åœ°æµ‹è¯•:\")\n    print(\"  âœ“ ä½¿ç”¨å°æ•°æ®é›†\")\n    print(\"  âœ“ å¿«é€Ÿè®­ç»ƒ (5 epochs)\")\n    print(\"  âœ“ å°æ¨¡å‹é…ç½®\")\n    print(\"  âœ“ å¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\")\nelif RUNNING_MODE == \"TRAIN\":\n    print(\"ğŸš€ TRAINæ¨¡å¼ - ç”¨äºäº‘ç«¯GPUè®­ç»ƒ:\")\n    print(\"  âœ“ ä½¿ç”¨å®Œæ•´æ•°æ®é›†\")\n    print(\"  âœ“ å®Œæ•´è®­ç»ƒ (100 epochs)\")\n    print(\"  âœ“ æ ‡å‡†æ¨¡å‹é…ç½®\")\n    print(\"  âœ“ æœ€ä½³æ€§èƒ½ä¼˜åŒ–\")\nelse:\n    raise ValueError(\"RUNNING_MODE å¿…é¡»æ˜¯ 'TEST' æˆ– 'TRAIN'\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 2. ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchç‰ˆæœ¬: 2.7.1\n",
      "CUDAå¯ç”¨: False\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3. æ•°æ®åŠ è½½å’Œæ¢ç´¢"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ğŸ“‹ æ ¹æ®è¿è¡Œæ¨¡å¼é…ç½®å‚æ•°\nif RUNNING_MODE == \"TEST\":\n    # ğŸ§ª TESTæ¨¡å¼é…ç½® - å¿«é€Ÿæµ‹è¯•ä»£ç æ­£ç¡®æ€§\n    config = {\n        # æ•°æ®å‚æ•° - ä½¿ç”¨æ›´å°çš„æ•°æ®é‡\n        'data_path': '../dataset',\n        'patch_size': 32,     # æ›´å°çš„patchå¤§å°\n        'stride': 16,         # æ›´å°çš„strideï¼Œå‡å°‘æ•°æ®é‡\n        'test_size': 0.3,     # æ›´å¤§çš„æµ‹è¯•é›†æ¯”ä¾‹\n        'val_size': 0.2,      # æ›´å¤§çš„éªŒè¯é›†æ¯”ä¾‹\n        'batch_size': 2,      # å°batch sizeç”¨äºæµ‹è¯•\n        'num_workers': 2,     # å°‘é‡å·¥ä½œè¿›ç¨‹\n        \n        # æ¨¡å‹å‚æ•° - è½»é‡çº§é…ç½®\n        'input_channels': 8,\n        'temporal_steps': 28,\n        'model_patch_size': 8,\n        'embed_dim': 128,     # æ›´å°çš„åµŒå…¥ç»´åº¦\n        'num_layers': 3,      # æ›´å°‘çš„å±‚æ•°\n        'num_heads': 4,       # æ›´å°‘çš„æ³¨æ„åŠ›å¤´\n        'mlp_ratio': 2.0,     # æ›´å°çš„MLPæ¯”ä¾‹\n        'dropout': 0.0,       # ç®€åŒ–é…ç½®\n        \n        # è®­ç»ƒå‚æ•° - å¿«é€Ÿè®­ç»ƒ\n        'epochs': 5,          # å¾ˆå°‘çš„epochsç”¨äºæµ‹è¯•\n        'learning_rate': 1e-3, # ç¨å¤§çš„å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›\n        'weight_decay': 0.01,\n        'gradient_accumulation_steps': 1,\n        'max_grad_norm': 1.0,\n        \n        # æŸå¤±å‡½æ•°\n        'use_combined_loss': False,  # ç®€å•æŸå¤±å‡½æ•°\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.05,\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        'scheduler_type': 'none',  # ä¸ä½¿ç”¨è°ƒåº¦å™¨\n        'min_lr': 1e-5,\n        \n        # å…¶ä»–\n        'patience': 5,        # çŸ­è€å¿ƒå€¼\n        'save_dir': './checkpoints_test',\n        'augment_train': False  # ä¸ä½¿ç”¨æ•°æ®å¢å¼º\n    }\n    print(\"ğŸ§ª ä½¿ç”¨TESTæ¨¡å¼é…ç½® - é€‚åˆå¿«é€ŸéªŒè¯ä»£ç \")\n    \nelif RUNNING_MODE == \"TRAIN\":\n    # ğŸš€ TRAINæ¨¡å¼é…ç½® - å®Œæ•´è®­ç»ƒæµç¨‹\n    config = {\n        # æ•°æ®å‚æ•° - ä½¿ç”¨å®Œæ•´æ•°æ®é›†\n        'data_path': '../dataset',\n        'patch_size': 64,     # æ ‡å‡†patchå¤§å°\n        'stride': 32,         # æ ‡å‡†stride\n        'test_size': 0.2,\n        'val_size': 0.1,\n        'batch_size': 8,      # é€‚ä¸­çš„batch size\n        'num_workers': 4,\n        \n        # æ¨¡å‹å‚æ•° - æ ‡å‡†é…ç½®\n        'input_channels': 8,\n        'temporal_steps': 28,\n        'model_patch_size': 8,\n        'embed_dim': 256,     # æ ‡å‡†åµŒå…¥ç»´åº¦\n        'num_layers': 6,      # æ ‡å‡†å±‚æ•°\n        'num_heads': 8,       # æ ‡å‡†æ³¨æ„åŠ›å¤´æ•°\n        'mlp_ratio': 4.0,     # æ ‡å‡†MLPæ¯”ä¾‹\n        'dropout': 0.1,\n        \n        # è®­ç»ƒå‚æ•° - å®Œæ•´è®­ç»ƒ\n        'epochs': 100,        # å®Œæ•´è®­ç»ƒè½®æ•°\n        'learning_rate': 1e-4, # æ ‡å‡†å­¦ä¹ ç‡\n        'weight_decay': 1e-4,\n        'gradient_accumulation_steps': 2,\n        'max_grad_norm': 1.0,\n        \n        # æŸå¤±å‡½æ•°\n        'use_combined_loss': True,  # ä½¿ç”¨ç»„åˆæŸå¤±\n        'focal_gamma': 2.5,\n        'label_smoothing': 0.1,\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        'scheduler_type': 'cosine',\n        'min_lr': 1e-6,\n        \n        # å…¶ä»–\n        'patience': 15,       # é•¿è€å¿ƒå€¼\n        'save_dir': './checkpoints_train',\n        'augment_train': True  # ä½¿ç”¨æ•°æ®å¢å¼º\n    }\n    print(\"ğŸš€ ä½¿ç”¨TRAINæ¨¡å¼é…ç½® - é€‚åˆå®Œæ•´è®­ç»ƒ\")\n\n# åˆ›å»ºä¿å­˜ç›®å½•\nPath(config['save_dir']).mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nğŸ“‹ å½“å‰é…ç½® ({RUNNING_MODE}æ¨¡å¼):\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")\n\nif RUNNING_MODE == \"TEST\":\n    print(\"\\nâš ï¸ æ³¨æ„: TESTæ¨¡å¼ä½¿ç”¨ç®€åŒ–é…ç½®ï¼Œä»…ç”¨äºéªŒè¯ä»£ç æ­£ç¡®æ€§!\")\nelse:\n    print(\"\\nğŸ’ª TRAINæ¨¡å¼: ä½¿ç”¨å®Œæ•´é…ç½®è¿›è¡Œæœ€ä½³æ€§èƒ½è®­ç»ƒ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”„ åŠ è½½å’Œå‡†å¤‡æ•°æ®ï¼ˆé€‚é…ä¸åŒæ¨¡å¼ï¼‰\nprint(f\"ğŸ”„ åŠ è½½æ•°æ® ({RUNNING_MODE}æ¨¡å¼)...\")\n\ntrain_loader, val_loader, test_loader, data_info = prepare_data(\n    data_path=config['data_path'],\n    patch_size=config['patch_size'],\n    stride=config['stride'],\n    test_size=config['test_size'],\n    val_size=config['val_size'],\n    batch_size=config['batch_size'],\n    num_workers=config['num_workers'],\n    augment_train=config['augment_train']\n)\n\nprint(f\"\\nğŸ“Š æ•°æ®é›†ä¿¡æ¯ ({RUNNING_MODE}æ¨¡å¼):\")\nprint(f\"  ç±»åˆ«æ•°é‡: {data_info['num_classes']}\")\nprint(f\"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}\")\nprint(f\"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)}\")\nprint(f\"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}\")\nprint(f\"  è¾“å…¥å½¢çŠ¶: {data_info['input_shape']}\")\n\n# ä¼°ç®—æ•°æ®é‡\ntotal_train_samples = len(train_loader) * config['batch_size']\ntotal_val_samples = len(val_loader) * config['batch_size']\nprint(f\"  è®­ç»ƒæ ·æœ¬æ•°: ~{total_train_samples}\")\nprint(f\"  éªŒè¯æ ·æœ¬æ•°: ~{total_val_samples}\")\n\nif RUNNING_MODE == \"TEST\":\n    print(f\"\\nğŸ§ª TESTæ¨¡å¼æ•°æ®é‡: ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•\")\n    print(f\"  é¢„è®¡å•epochæ—¶é—´: ~30ç§’\")\n    print(f\"  æ€»è®­ç»ƒæ—¶é—´: ~3åˆ†é’Ÿ\")\nelse:\n    print(f\"\\nğŸš€ TRAINæ¨¡å¼æ•°æ®é‡: ä½¿ç”¨å®Œæ•´æ•°æ®é›†\")\n    print(f\"  é¢„è®¡å•epochæ—¶é—´: ~5-10åˆ†é’Ÿ\")\n    print(f\"  æ€»è®­ç»ƒæ—¶é—´: ~8-16å°æ—¶\")\n\n# ä¿å­˜æ•°æ®ä¿¡æ¯\nsave_data_info(data_info, f\"{config['save_dir']}/data_info.pkl\")\n\n# æ˜¾ç¤ºç±»åˆ«ä¿¡æ¯\nprint(f\"\\nğŸ·ï¸ ç±»åˆ«ä¿¡æ¯:\")\nfor idx, name in data_info['class_names'].items():\n    print(f\"  {idx}: {name}\")\n\nprint(f\"\\nâš–ï¸ ç±»åˆ«æƒé‡: {data_info['class_weights'].numpy()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“‹ æ ¹æ®è¿è¡Œæ¨¡å¼é…ç½®å‚æ•°\nif RUNNING_MODE == \"TEST\":\n    # ğŸ§ª TESTæ¨¡å¼é…ç½® - å¿«é€Ÿæµ‹è¯•ä»£ç æ­£ç¡®æ€§\n    config = {\n        # æ•°æ®å‚æ•° - ä½¿ç”¨æ›´å°çš„æ•°æ®é‡\n        'data_path': '../dataset',\n        'patch_size': 32,     # æ›´å°çš„patchå¤§å°\n        'stride': 16,         # æ›´å°çš„strideï¼Œå‡å°‘æ•°æ®é‡\n        'test_size': 0.3,     # æ›´å¤§çš„æµ‹è¯•é›†æ¯”ä¾‹\n        'val_size': 0.2,      # æ›´å¤§çš„éªŒè¯é›†æ¯”ä¾‹\n        'batch_size': 2,      # å°batch sizeç”¨äºæµ‹è¯•\n        'num_workers': 2,     # å°‘é‡å·¥ä½œè¿›ç¨‹\n        \n        # æ¨¡å‹å‚æ•° - è½»é‡çº§é…ç½®\n        'input_channels': 8,\n        'temporal_steps': 28,\n        'model_patch_size': 8,\n        'embed_dim': 128,     # æ›´å°çš„åµŒå…¥ç»´åº¦\n        'num_layers': 3,      # æ›´å°‘çš„å±‚æ•°\n        'num_heads': 4,       # æ›´å°‘çš„æ³¨æ„åŠ›å¤´\n        'mlp_ratio': 2.0,     # æ›´å°çš„MLPæ¯”ä¾‹\n        'dropout': 0.0,       # ç®€åŒ–é…ç½®\n        \n        # è®­ç»ƒå‚æ•° - å¿«é€Ÿè®­ç»ƒ\n        'epochs': 5,          # å¾ˆå°‘çš„epochsç”¨äºæµ‹è¯•\n        'learning_rate': 1e-3, # ç¨å¤§çš„å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›\n        'weight_decay': 0.01,\n        'gradient_accumulation_steps': 1,\n        'max_grad_norm': 1.0,\n        \n        # æŸå¤±å‡½æ•°\n        'use_combined_loss': False,  # ç®€å•æŸå¤±å‡½æ•°\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.05,\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        'scheduler_type': 'none',  # ä¸ä½¿ç”¨è°ƒåº¦å™¨\n        'min_lr': 1e-5,\n        \n        # å…¶ä»–\n        'patience': 5,        # çŸ­è€å¿ƒå€¼\n        'save_dir': '../Models/vision-transformer/test',  # ä¿®æ”¹è¾“å‡ºè·¯å¾„\n        'augment_train': False  # ä¸ä½¿ç”¨æ•°æ®å¢å¼º\n    }\n    print(\"ğŸ§ª ä½¿ç”¨TESTæ¨¡å¼é…ç½® - é€‚åˆå¿«é€ŸéªŒè¯ä»£ç \")\n    \nelif RUNNING_MODE == \"TRAIN\":\n    # ğŸš€ TRAINæ¨¡å¼é…ç½® - å®Œæ•´è®­ç»ƒæµç¨‹\n    config = {\n        # æ•°æ®å‚æ•° - ä½¿ç”¨å®Œæ•´æ•°æ®é›†\n        'data_path': '../dataset',\n        'patch_size': 64,     # æ ‡å‡†patchå¤§å°\n        'stride': 32,         # æ ‡å‡†stride\n        'test_size': 0.2,\n        'val_size': 0.1,\n        'batch_size': 8,      # é€‚ä¸­çš„batch size\n        'num_workers': 4,\n        \n        # æ¨¡å‹å‚æ•° - æ ‡å‡†é…ç½®\n        'input_channels': 8,\n        'temporal_steps': 28,\n        'model_patch_size': 8,\n        'embed_dim': 256,     # æ ‡å‡†åµŒå…¥ç»´åº¦\n        'num_layers': 6,      # æ ‡å‡†å±‚æ•°\n        'num_heads': 8,       # æ ‡å‡†æ³¨æ„åŠ›å¤´æ•°\n        'mlp_ratio': 4.0,     # æ ‡å‡†MLPæ¯”ä¾‹\n        'dropout': 0.1,\n        \n        # è®­ç»ƒå‚æ•° - å®Œæ•´è®­ç»ƒ\n        'epochs': 100,        # å®Œæ•´è®­ç»ƒè½®æ•°\n        'learning_rate': 1e-4, # æ ‡å‡†å­¦ä¹ ç‡\n        'weight_decay': 1e-4,\n        'gradient_accumulation_steps': 2,\n        'max_grad_norm': 1.0,\n        \n        # æŸå¤±å‡½æ•°\n        'use_combined_loss': True,  # ä½¿ç”¨ç»„åˆæŸå¤±\n        'focal_gamma': 2.5,\n        'label_smoothing': 0.1,\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        'scheduler_type': 'cosine',\n        'min_lr': 1e-6,\n        \n        # å…¶ä»–\n        'patience': 15,       # é•¿è€å¿ƒå€¼\n        'save_dir': '../Models/vision-transformer/train',  # ä¿®æ”¹è¾“å‡ºè·¯å¾„\n        'augment_train': True  # ä½¿ç”¨æ•°æ®å¢å¼º\n    }\n    print(\"ğŸš€ ä½¿ç”¨TRAINæ¨¡å¼é…ç½® - é€‚åˆå®Œæ•´è®­ç»ƒ\")\n\n# åˆ›å»ºä¿å­˜ç›®å½•\nPath(config['save_dir']).mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nğŸ“‹ å½“å‰é…ç½® ({RUNNING_MODE}æ¨¡å¼):\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")\n\nif RUNNING_MODE == \"TEST\":\n    print(\"\\nâš ï¸ æ³¨æ„: TESTæ¨¡å¼ä½¿ç”¨ç®€åŒ–é…ç½®ï¼Œä»…ç”¨äºéªŒè¯ä»£ç æ­£ç¡®æ€§!\")\nelse:\n    print(\"\\nğŸ’ª TRAINæ¨¡å¼: ä½¿ç”¨å®Œæ•´é…ç½®è¿›è¡Œæœ€ä½³æ€§èƒ½è®­ç»ƒ!\")\n\nprint(f\"\\nğŸ’¾ æ¨¡å‹è¾“å‡ºç›®å½•: {config['save_dir']}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. æ¨¡å‹åˆ›å»ºå’Œé…ç½®"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹åˆ›å»ºå’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®¾å¤‡\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# åˆ›å»ºTransformeræ¨¡å‹\n",
    "model = create_transformer_model(\n",
    "    input_channels=config['input_channels'],\n",
    "    temporal_steps=config['temporal_steps'],\n",
    "    num_classes=data_info['num_classes'],\n",
    "    patch_size=config['model_patch_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    num_layers=config['num_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# è®¡ç®—å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nğŸ”§ æ¨¡å‹ä¿¡æ¯:\")\n",
    "print(f\"  æ€»å‚æ•°é‡: {total_params:,}\")\n",
    "print(f\"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "print(f\"  æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.1f} MB (FP32)\")\n",
    "\n",
    "# æµ‹è¯•å‰å‘ä¼ æ’­\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = sample_batch_x[:2].to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"\\nğŸ§ª å‰å‘ä¼ æ’­æµ‹è¯•:\")\n",
    "    print(f\"  è¾“å…¥å½¢çŠ¶: {test_input.shape}\")\n",
    "    print(f\"  è¾“å‡ºå½¢çŠ¶: {test_output.shape}\")\n",
    "    print(f\"  è¾“å‡ºæ•°å€¼èŒƒå›´: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "\n",
    "print(\"\\nâœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. è®­ç»ƒè¿‡ç¨‹"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ğŸš€ è®­ç»ƒå‡†å¤‡ï¼ˆé€‚é…ä¸åŒæ¨¡å¼ï¼‰\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'val_miou': [],\n    'learning_rates': []\n}\n\nbest_miou = 0.0\nstart_time = time.time()\n\nif RUNNING_MODE == \"TEST\":\n    print(f\"ğŸ§ª å¼€å§‹TESTæ¨¡å¼è®­ç»ƒ - Vision Transformer ({config['epochs']} epochs)\")\n    print(f\"âš¡ ç›®æ ‡: å¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\")\n    print(f\"ğŸ”§ æ¨¡å‹é…ç½®: è½»é‡çº§ (embed_dim={config['embed_dim']}, layers={config['num_layers']})\")\nelse:\n    print(f\"ğŸš€ å¼€å§‹TRAINæ¨¡å¼è®­ç»ƒ - Vision Transformer ({config['epochs']} epochs)\")\n    print(f\"ğŸ¯ ç›®æ ‡: è·å¾—æœ€ä½³æ¨¡å‹æ€§èƒ½\")\n    print(f\"ğŸ’ª æ¨¡å‹é…ç½®: æ ‡å‡†é…ç½® (embed_dim={config['embed_dim']}, layers={config['num_layers']})\")\n\nprint(f\"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"ğŸ¯ ç‰¹å¾: {config['num_layers']}å±‚æ·±åº¦, {config['embed_dim']}åµŒå…¥ç»´åº¦, {config['num_heads']}æ³¨æ„åŠ›å¤´\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå†å²è®°å½•\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_miou': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_miou = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"ğŸš€ å¼€å§‹è®­ç»ƒ Transformer æ¨¡å‹ ({config['epochs']} epochs)\")\n",
    "print(f\"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{config['epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # è®­ç»ƒé˜¶æ®µ\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, scaler, epoch,\n",
    "        scheduler=None,  # æˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨scheduler\n",
    "        scheduler_step_per_batch=False,\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        max_grad_norm=config['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    # éªŒè¯é˜¶æ®µ\n",
    "    val_loss, val_acc, val_metrics = validate(\n",
    "        model, val_loader, criterion, device, data_info['num_classes']\n",
    "    )\n",
    "    \n",
    "    # å­¦ä¹ ç‡è°ƒåº¦\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # è·å–å½“å‰å­¦ä¹ ç‡\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # è®°å½•å†å²\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_miou'].append(val_metrics['mean_iou'])\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # è®¡ç®—epochæ—¶é—´\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(f\"Epoch {epoch} ç»“æœ:\")\n",
    "    print(f\"  è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  éªŒè¯ mIoU: {val_metrics['mean_iou']:.4f}\")\n",
    "    print(f\"  å­¦ä¹ ç‡: {current_lr:.2e}\")\n",
    "    print(f\"  è€—æ—¶: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    if val_metrics['mean_iou'] > best_miou:\n",
    "        best_miou = val_metrics['mean_iou']\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_miou': best_miou,\n",
    "            'config': config,\n",
    "            'metrics': val_metrics\n",
    "        }, f\"{config['save_dir']}/best_model.pth\")\n",
    "        print(f\"  ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! (mIoU: {best_miou:.4f})\")\n",
    "    \n",
    "    # æ—©åœæ£€æŸ¥\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"\\nâ¹ï¸ è§¦å‘æ—©åœ!\")\n",
    "        break\n",
    "    \n",
    "    # æ¯5ä¸ªepochæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦\n",
    "    if epoch % 5 == 0:\n",
    "        total_time = time.time() - start_time\n",
    "        avg_epoch_time = total_time / epoch\n",
    "        estimated_total = avg_epoch_time * config['epochs']\n",
    "        print(f\"  ğŸ“Š è¿›åº¦: {epoch/config['epochs']*100:.1f}%, é¢„è®¡å‰©ä½™æ—¶é—´: {(estimated_total - total_time)/60:.1f}åˆ†é’Ÿ\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
    "print(f\"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ\")\n",
    "print(f\"ğŸ† æœ€ä½³ mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. æ¨¡å‹è¯„ä¼°"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°\n",
    "print(\"ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°...\")\n",
    "\n",
    "# é‡æ–°åˆ›å»ºæ¨¡å‹\n",
    "eval_model = create_transformer_model(\n",
    "    input_channels=config['input_channels'],\n",
    "    temporal_steps=config['temporal_steps'],\n",
    "    num_classes=data_info['num_classes'],\n",
    "    patch_size=config['model_patch_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    num_layers=config['num_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# åŠ è½½æœ€ä½³æƒé‡\n",
    "checkpoint = load_checkpoint(f\"{config['save_dir']}/best_model.pth\", eval_model)\n",
    "eval_model.eval()\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒepoch: {checkpoint['epoch']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "print(\"ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...\")\n",
    "\n",
    "eval_model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets_cpu = targets.numpy()\n",
    "        \n",
    "        outputs = eval_model(data)  # (batch, height, width, num_classes)\n",
    "        probs = torch.softmax(outputs, dim=-1)\n",
    "        _, predicted = outputs.max(-1)\n",
    "        \n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_targets.append(targets_cpu)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡\n",
    "all_preds = np.concatenate(all_preds).flatten()\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "\n",
    "# è®¡ç®—è¯¦ç»†æŒ‡æ ‡\n",
    "metrics = calculate_metrics(all_preds, all_targets, data_info['num_classes'])\n",
    "metrics['class_names'] = data_info['class_names']\n",
    "\n",
    "print(f\"\\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:\")\n",
    "print(f\"  æ€»ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}\")\n",
    "print(f\"  å¹³å‡å‡†ç¡®ç‡: {metrics['mean_accuracy']:.4f}\")\n",
    "print(f\"  å¹³å‡IoU: {metrics['mean_iou']:.4f}\")\n",
    "print(f\"  å¹³å‡ç²¾ç¡®ç‡: {metrics['mean_precision']:.4f}\")\n",
    "print(f\"  å¹³å‡å¬å›ç‡: {metrics['mean_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. ç»“æœå¯è§†åŒ–"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç»“æœå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "plot_confusion_matrix(\n",
    "    metrics['confusion_matrix'],\n",
    "    list(data_info['class_names'].values()),\n",
    "    save_path=f\"{config['save_dir']}/confusion_matrix.png\",\n",
    "    title=\"Transformer Confusion Matrix\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å›¾è¡¨\n",
    "plot_class_performance(\n",
    "    metrics, \n",
    "    save_path=f\"{config['save_dir']}/class_performance.png\",\n",
    "    title=\"Transformer Per-Class Performance\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ˆ ç±»åˆ«æ€§èƒ½å›¾è¡¨å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. æ³¨æ„åŠ›æœºåˆ¶åˆ†æ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºå•ä¸ªpatchçš„æ¨ç†\n",
    "print(\"ğŸ”® æ¼”ç¤ºæ¨¡å‹æ¨ç†...\")\n",
    "\n",
    "# è·å–ä¸€ä¸ªæ ·æœ¬\n",
    "sample_x = test_batch_x[0:1].to(device)  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "sample_y = test_batch_y[0:1]\n",
    "\n",
    "# æ¨ç†\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    inference_start = time.time()\n",
    "    outputs = eval_model(sample_x)\n",
    "    inference_time = time.time() - inference_start\n",
    "    \n",
    "    # è·å–é¢„æµ‹å’Œç½®ä¿¡åº¦\n",
    "    probs = torch.softmax(outputs, dim=-1)\n",
    "    confidence, predictions = probs.max(-1)\n",
    "    \n",
    "    # è®¡ç®—å‡†ç¡®ç‡\n",
    "    accuracy = (predictions.cpu() == sample_y).float().mean().item()\n",
    "\n",
    "print(f\"\\nâš¡ æ¨ç†æ€§èƒ½:\")\n",
    "print(f\"  æ¨ç†æ—¶é—´: {inference_time*1000:.2f}ms\")\n",
    "print(f\"  è¾“å…¥å½¢çŠ¶: {sample_x.shape}\")\n",
    "print(f\"  è¾“å‡ºå½¢çŠ¶: {outputs.shape}\")\n",
    "print(f\"  é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "print(f\"  å¹³å‡ç½®ä¿¡åº¦: {confidence.mean():.4f}\")\n",
    "print(f\"  æœ€å°ç½®ä¿¡åº¦: {confidence.min():.4f}\")\n",
    "print(f\"  æœ€å¤§ç½®ä¿¡åº¦: {confidence.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 9. æ¨¡å‹æ¨ç†"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 80)\nprint(f\"ğŸ¯ VISION TRANSFORMER å†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒæ€»ç»“ ({RUNNING_MODE}æ¨¡å¼)\")\nprint(\"=\" * 80)\n\nprint(f\"\\nğŸ”„ è¿è¡Œæ¨¡å¼: {RUNNING_MODE}\")\nif RUNNING_MODE == \"TEST\":\n    print(\"  âœ“ å¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\")\n    print(\"  âœ“ ä½¿ç”¨è½»é‡çº§æ¨¡å‹é…ç½®\")\n    print(\"  âœ“ å°‘é‡è®­ç»ƒè½®æ•°\")\n    print(\"  âœ“ é€‚åˆæœ¬åœ°å¼€å‘ç¯å¢ƒ\")\nelse:\n    print(\"  âœ“ å®Œæ•´è®­ç»ƒæµç¨‹\")\n    print(\"  âœ“ æ ‡å‡†æ¨¡å‹é…ç½®\")\n    print(\"  âœ“ å……è¶³è®­ç»ƒè½®æ•°\")\n    print(\"  âœ“ é€‚åˆäº‘ç«¯GPUç¯å¢ƒ\")\n\nprint(f\"\\nğŸ“Š æ¨¡å‹é…ç½®:\")\nprint(f\"  æ¨¡å‹ç±»å‹: Vision Transformer\")\nprint(f\"  åµŒå…¥ç»´åº¦: {config['embed_dim']}\")\nprint(f\"  å±‚æ•°: {config['num_layers']}\")\nprint(f\"  æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}\")\nprint(f\"  Patchå¤§å°: {config['model_patch_size']}x{config['model_patch_size']}\")\nprint(f\"  æ€»å‚æ•°é‡: {total_params:,}\")\n\nprint(f\"\\nğŸƒ è®­ç»ƒè¿‡ç¨‹:\")\nprint(f\"  è®­ç»ƒepochs: {len(history['train_loss'])}\")\nprint(f\"  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ\")\nprint(f\"  å¹³å‡æ¯epoch: {total_training_time/len(history['train_loss']):.1f}ç§’\")\nprint(f\"  æœ€ä½³éªŒè¯mIoU: {best_miou:.4f}\")\nif len(history['learning_rates']) > 0:\n    print(f\"  æœ€ç»ˆå­¦ä¹ ç‡: {history['learning_rates'][-1]:.2e}\")\n\nprint(f\"\\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½:\")\nprint(f\"  æµ‹è¯•é›†æ€»ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}\")\nprint(f\"  æµ‹è¯•é›†å¹³å‡IoU: {metrics['mean_iou']:.4f}\")\nprint(f\"  æµ‹è¯•é›†å¹³å‡ç²¾ç¡®ç‡: {metrics['mean_precision']:.4f}\")\nprint(f\"  æµ‹è¯•é›†å¹³å‡å¬å›ç‡: {metrics['mean_recall']:.4f}\")\n\nprint(f\"\\nğŸ¯ Vision Transformerç‰¹è‰²:\")\nprint(f\"  âœ“ å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶: å»ºç«‹é•¿ç¨‹ä¾èµ–å…³ç³»\")\nprint(f\"  âœ“ ä½ç½®ç¼–ç : æä¾›ç©ºé—´ä½ç½®ä¿¡æ¯\")\nprint(f\"  âœ“ å¤šå¤´æ³¨æ„åŠ›: æ•è·ä¸åŒç±»å‹çš„ç‰¹å¾å…³ç³»\")\nprint(f\"  âœ“ æ—¶ç©ºèåˆ: æœ‰æ•ˆå¤„ç†å¤šå…‰è°±æ—¶åºæ•°æ®\")\n\nprint(f\"\\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:\")\nsave_dir = Path(config['save_dir'])\nsaved_files = list(save_dir.glob('*'))\nfor file in saved_files:\n    print(f\"  {file.name}\")\n\nprint(f\"\\nğŸš€ ä½¿ç”¨å»ºè®®:\")\nif RUNNING_MODE == \"TEST\":\n    print(f\"  âœ… ä»£ç æµ‹è¯•å®Œæˆï¼Œå¯ä»¥åˆ‡æ¢åˆ°TRAINæ¨¡å¼è¿›è¡Œå®Œæ•´è®­ç»ƒ\")\n    print(f\"  ğŸ“ ä¿®æ”¹ç¬¬ä¸€ä¸ªcell: RUNNING_MODE = 'TRAIN'\")\n    print(f\"  ğŸ”„ é‡æ–°è¿è¡Œnotebookè¿›è¡Œå®Œæ•´è®­ç»ƒ\")\n    print(f\"  â˜ï¸ å»ºè®®åœ¨äº‘ç«¯GPUä¸Šè¿è¡ŒTRAINæ¨¡å¼\")\nelse:\n    print(f\"  1. æ¨¡å‹å·²ä¿å­˜åˆ°: {config['save_dir']}/best_model.pth\")\n    print(f\"  2. ç”¨äºæ¨ç†: python -m Transformer.inference --model-path {config['save_dir']}/best_model.pth\")\n    print(f\"  3. æ³¨æ„åŠ›åˆ†æ: å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ\")\n    print(f\"  4. æ¨¡å‹éƒ¨ç½²: è€ƒè™‘å¯¼å‡ºä¸ºONNXæ ¼å¼\")\n\nprint(f\"\\nğŸ“Š ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”:\")\nprint(f\"  vs TCN: æ›´å¼ºçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›\")\nprint(f\"  vs Swin Transformer: æ›´ç›´æ¥çš„å…¨å±€æ³¨æ„åŠ›ä½†è®¡ç®—å¤æ‚åº¦æ›´é«˜\")\nprint(f\"  ç‰¹ç‚¹: çº¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼ºå¤§çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›\")\n\nprint(\"\\n\" + \"=\" * 80)\nif RUNNING_MODE == \"TEST\":\n    print(\"ğŸ§ª Vision Transformerä»£ç æµ‹è¯•å®Œæˆ!\")\n    print(\"âœ… æ‰€æœ‰ç»„ä»¶è¿è¡Œæ­£å¸¸ï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´è®­ç»ƒ!\")\nelse:\n    print(\"ğŸ‰ Vision Transformerå†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒå®Œæˆ!\")\n    print(\"ğŸŒŸ å…¨å±€è‡ªæ³¨æ„åŠ›ï¼Œå¼ºå¤§ç‰¹å¾å­¦ä¹ ï¼Œä¼˜ç§€æ³›åŒ–æ€§èƒ½!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ TRANSFORMER å†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒæ€»ç»“\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹é…ç½®:\")\n",
    "print(f\"  æ¨¡å‹ç±»å‹: Vision Transformer\")\n",
    "print(f\"  åµŒå…¥ç»´åº¦: {config['embed_dim']}\")\n",
    "print(f\"  å±‚æ•°: {config['num_layers']}\")\n",
    "print(f\"  æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}\")\n",
    "print(f\"  æ€»å‚æ•°é‡: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nğŸƒ è®­ç»ƒè¿‡ç¨‹:\")\n",
    "print(f\"  è®­ç»ƒepochs: {len(history['train_loss'])}\")\n",
    "print(f\"  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ\")\n",
    "print(f\"  å¹³å‡æ¯epoch: {total_training_time/len(history['train_loss']):.1f}ç§’\")\n",
    "print(f\"  æœ€ä½³éªŒè¯mIoU: {best_miou:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½:\")\n",
    "print(f\"  æµ‹è¯•é›†æ€»ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}\")\n",
    "print(f\"  æµ‹è¯•é›†å¹³å‡IoU: {metrics['mean_iou']:.4f}\")\n",
    "print(f\"  æµ‹è¯•é›†å¹³å‡ç²¾ç¡®ç‡: {metrics['mean_precision']:.4f}\")\n",
    "print(f\"  æµ‹è¯•é›†å¹³å‡å¬å›ç‡: {metrics['mean_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:\")\n",
    "save_dir = Path(config['save_dir'])\n",
    "saved_files = list(save_dir.glob('*'))\n",
    "for file in saved_files:\n",
    "    print(f\"  {file.name}\")\n",
    "\n",
    "print(f\"\\nğŸš€ ä½¿ç”¨å»ºè®®:\")\n",
    "print(f\"  1. æ¨¡å‹å·²ä¿å­˜åˆ°: {config['save_dir']}/best_model.pth\")\n",
    "print(f\"  2. ç”¨äºæ¨ç†: python -m Transformer.inference --model-path {config['save_dir']}/best_model.pth\")\n",
    "print(f\"  3. ç»§ç»­è®­ç»ƒ: åŠ è½½checkpointå¹¶è°ƒæ•´å­¦ä¹ ç‡\")\n",
    "print(f\"  4. æ¨¡å‹éƒ¨ç½²: è€ƒè™‘é‡åŒ–æˆ–å‰ªæä»¥å‡å°‘è®¡ç®—éœ€æ±‚\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ Transformerå†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒå®Œæˆ!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepCropMapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}