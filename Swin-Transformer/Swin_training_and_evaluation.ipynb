{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Swin Transformer å†œä½œç‰©åˆ¶å›¾è®­ç»ƒå’Œè¯„ä¼°\\n\\næœ¬notebookæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Swin Transformeræ¨¡å‹è¿›è¡Œå¤šå…‰è°±æ—¶åºæ•°æ®çš„å†œä½œç‰©åˆ†ç±»ä»»åŠ¡ã€‚Swin Transformeré‡‡ç”¨å±‚æ¬¡åŒ–è®¾è®¡å’Œç§»ä½çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†çº¿æ€§è®¡ç®—å¤æ‚åº¦ã€‚\\n\\n## ğŸ”„ åŒæ¨¡å¼è®¾è®¡\\n\\n**ğŸ“‹ æœ¬notebookæ”¯æŒä¸¤ç§è¿è¡Œæ¨¡å¼ï¼š**\\n\\n### ğŸ§ª TESTæ¨¡å¼ \\n- **ç›®çš„**: æœ¬åœ°å¿«é€Ÿæµ‹è¯•ä»£ç æ­£ç¡®æ€§\\n- **é…ç½®**: è½»é‡çº§æ¨¡å‹ï¼Œå°æ•°æ®é›†ï¼Œå°‘é‡epochs\\n- **ç”¨æ—¶**: ~3åˆ†é’Ÿ\\n- **é€‚ç”¨**: å¼€å‘è°ƒè¯•ï¼ŒéªŒè¯ä»£ç é€»è¾‘\\n- **ç¯å¢ƒ**: æœ¬åœ°CPU/GPUï¼Œä½å†…å­˜è¦æ±‚\\n\\n### ğŸš€ TRAINæ¨¡å¼\\n- **ç›®çš„**: äº‘ç«¯GPUå®Œæ•´è®­ç»ƒè·å¾—æœ€ä½³æ€§èƒ½\\n- **é…ç½®**: æ ‡å‡†æ¨¡å‹ï¼Œå®Œæ•´æ•°æ®é›†ï¼Œå®Œæ•´epochs\\n- **ç”¨æ—¶**: ~8-16å°æ—¶\\n- **é€‚ç”¨**: æ­£å¼è®­ç»ƒï¼Œè·å¾—éƒ¨ç½²æ¨¡å‹\\n- **ç¯å¢ƒ**: äº‘ç«¯GPUï¼Œé«˜æ€§èƒ½è¦æ±‚\\n\\n**ğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š**\\n1. æœ¬åœ°å¼€å‘æ—¶ä½¿ç”¨TESTæ¨¡å¼éªŒè¯ä»£ç \\n2. ç¡®è®¤æ— è¯¯ååˆ‡æ¢åˆ°TRAINæ¨¡å¼è¿›è¡Œå®Œæ•´è®­ç»ƒ\\n3. åœ¨äº‘ç«¯GPUä¸Šè¿è¡ŒTRAINæ¨¡å¼ä»¥è·å¾—æœ€ä½³æ•ˆæœ\\n\\n---\\n\\n## ç›®å½•\\n1. [è¿è¡Œæ¨¡å¼é…ç½®](#è¿è¡Œæ¨¡å¼é…ç½®)\\n2. [ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥](#ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥)\\n3. [æ•°æ®åŠ è½½å’Œæ¢ç´¢](#æ•°æ®åŠ è½½å’Œæ¢ç´¢)\\n4. [æ¨¡å‹åˆ›å»ºå’Œé…ç½®](#æ¨¡å‹åˆ›å»ºå’Œé…ç½®)\\n5. [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)\\n6. [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)\\n7. [ç»“æœå¯è§†åŒ–](#ç»“æœå¯è§†åŒ–)\\n8. [çª—å£æ³¨æ„åŠ›åˆ†æ](#çª—å£æ³¨æ„åŠ›åˆ†æ)\\n9. [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ğŸ”§ è¿è¡Œæ¨¡å¼é…ç½®\n# è®¾ç½® RUNNING_MODE æ¥é€‰æ‹©è¿è¡Œæ¨¡å¼\n# - \"TEST\": æœ¬åœ°æµ‹è¯•æ¨¡å¼ï¼Œå¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\n# - \"TRAIN\": äº‘ç«¯è®­ç»ƒæ¨¡å¼ï¼Œå®Œæ•´è®­ç»ƒæµç¨‹\n\nRUNNING_MODE = \"TEST\"  # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œæ¥åˆ‡æ¢æ¨¡å¼: \"TEST\" æˆ– \"TRAIN\"\n\nprint(f\"ğŸ”„ å½“å‰è¿è¡Œæ¨¡å¼: {RUNNING_MODE}\")\n\nif RUNNING_MODE == \"TEST\":\n    print(\"ğŸ§ª TESTæ¨¡å¼ - ç”¨äºæœ¬åœ°æµ‹è¯•:\")\n    print(\"  âœ“ ä½¿ç”¨å°æ•°æ®é›†\")\n    print(\"  âœ“ å¿«é€Ÿè®­ç»ƒ (5 epochs)\")\n    print(\"  âœ“ å°æ¨¡å‹é…ç½®\")\n    print(\"  âœ“ å¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\")\nelif RUNNING_MODE == \"TRAIN\":\n    print(\"ğŸš€ TRAINæ¨¡å¼ - ç”¨äºäº‘ç«¯GPUè®­ç»ƒ:\")\n    print(\"  âœ“ ä½¿ç”¨å®Œæ•´æ•°æ®é›†\")\n    print(\"  âœ“ å®Œæ•´è®­ç»ƒ (50+ epochs)\")\n    print(\"  âœ“ æ ‡å‡†æ¨¡å‹é…ç½®\")\n    print(\"  âœ“ æœ€ä½³æ€§èƒ½ä¼˜åŒ–\")\nelse:\n    raise ValueError(\"RUNNING_MODE å¿…é¡»æ˜¯ 'TEST' æˆ– 'TRAIN'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. è¿è¡Œæ¨¡å¼é…ç½®\n\né€‰æ‹©è¿è¡Œæ¨¡å¼æ¥é€‚é…ä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼š\n- **TESTæ¨¡å¼**ï¼šå¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§ï¼Œé€‚åˆæœ¬åœ°å¼€å‘\n- **TRAINæ¨¡å¼**ï¼šå®Œæ•´è®­ç»ƒæµç¨‹ï¼Œé€‚åˆäº‘ç«¯GPUè®­ç»ƒ",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer å†œä½œç‰©åˆ¶å›¾è®­ç»ƒå’Œè¯„ä¼°\n",
    "\n",
    "æœ¬notebookæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Swin Transformeræ¨¡å‹è¿›è¡Œå¤šå…‰è°±æ—¶åºæ•°æ®çš„å†œä½œç‰©åˆ†ç±»ä»»åŠ¡ã€‚Swin Transformeré‡‡ç”¨å±‚æ¬¡åŒ–è®¾è®¡å’Œç§»ä½çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†çº¿æ€§è®¡ç®—å¤æ‚åº¦ã€‚\n",
    "\n",
    "## ç›®å½•\n",
    "1. [ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥](#ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥)\n",
    "2. [æ•°æ®åŠ è½½å’Œæ¢ç´¢](#æ•°æ®åŠ è½½å’Œæ¢ç´¢)\n",
    "3. [æ¨¡å‹åˆ›å»ºå’Œé…ç½®](#æ¨¡å‹åˆ›å»ºå’Œé…ç½®)\n",
    "4. [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)\n",
    "5. [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)\n",
    "6. [ç»“æœå¯è§†åŒ–](#ç»“æœå¯è§†åŒ–)\n",
    "7. [çª—å£æ³¨æ„åŠ›åˆ†æ](#çª—å£æ³¨æ„åŠ›åˆ†æ)\n",
    "8. [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®å’Œå¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰einopsåº“ï¼ˆSwin Transformeréœ€è¦ï¼‰\n",
    "try:\n",
    "    import einops\n",
    "    print(\"âœ“ einops available\")\n",
    "except ImportError:\n",
    "    print(\"âŒ einops not found. Please install: pip install einops\")\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥Swin Transformeræ¨¡å—\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model import create_swin_model, CropMappingSwinTransformer\n",
    "from dataset import prepare_data, save_data_info, load_data_info\n",
    "from train import (\n",
    "    train_epoch, validate, FocalLoss, DiceLoss, CombinedLoss\n",
    ")\n",
    "from utils import (\n",
    "    save_checkpoint, load_checkpoint, EarlyStopping,\n",
    "    calculate_metrics, plot_training_history, plot_class_performance,\n",
    "    plot_confusion_matrix, visualize_predictions, plot_window_attention\n",
    ")\n",
    "\n",
    "print(\"âœ… Swin Transformeræ¨¡å—å¯¼å…¥æˆåŠŸ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®åŠ è½½å’Œæ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“‹ æ ¹æ®è¿è¡Œæ¨¡å¼é…ç½®å‚æ•°\nif RUNNING_MODE == \"TEST\":\n    # ğŸ§ª TESTæ¨¡å¼é…ç½® - å¿«é€Ÿæµ‹è¯•ä»£ç æ­£ç¡®æ€§\n    config = {\n        # æ•°æ®å‚æ•° - ä½¿ç”¨æ›´å°çš„æ•°æ®é‡\n        'data_path': '../dataset',\n        'patch_size': 32,     # æ›´å°çš„patchå¤§å°\n        'stride': 16,         # æ›´å°çš„strideï¼Œå‡å°‘æ•°æ®é‡\n        'test_size': 0.3,     # æ›´å¤§çš„æµ‹è¯•é›†æ¯”ä¾‹\n        'val_size': 0.2,      # æ›´å¤§çš„éªŒè¯é›†æ¯”ä¾‹\n        'batch_size': 2,      # å°batch sizeç”¨äºæµ‹è¯•\n        'num_workers': 2,     # å°‘é‡å·¥ä½œè¿›ç¨‹\n        \n        # æ¨¡å‹å‚æ•° - è½»é‡çº§é…ç½®\n        'input_channels': 8,\n        'temporal_steps': 28,\n        'swin_patch_size': 4,\n        'embed_dim': 48,      # æ›´å°çš„åµŒå…¥ç»´åº¦\n        'depths': [1, 1, 2, 1],  # æ›´æµ…çš„ç½‘ç»œ\n        'num_heads': [2, 4, 8, 16],  # æ›´å°‘çš„æ³¨æ„åŠ›å¤´\n        'window_size': 4,     # æ›´å°çš„çª—å£\n        'mlp_ratio': 2.0,     # æ›´å°çš„MLPæ¯”ä¾‹\n        'dropout': 0.0,\n        'attn_dropout': 0.0,\n        'drop_path_rate': 0.05,\n        \n        # è®­ç»ƒå‚æ•° - å¿«é€Ÿè®­ç»ƒ\n        'epochs': 5,          # å¾ˆå°‘çš„epochsç”¨äºæµ‹è¯•\n        'learning_rate': 1e-3, # ç¨å¤§çš„å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›\n        'weight_decay': 0.01,\n        'gradient_accumulation_steps': 1,\n        'max_grad_norm': 1.0,\n        \n        # æŸå¤±å‡½æ•°\n        'use_combined_loss': False,  # ç®€å•æŸå¤±å‡½æ•°\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.05,\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        'scheduler_type': 'none',  # ä¸ä½¿ç”¨è°ƒåº¦å™¨\n        'min_lr': 1e-5,\n        \n        # å…¶ä»–\n        'patience': 5,        # çŸ­è€å¿ƒå€¼\n        'save_dir': '../Models/swin-transformer/test',  # ä¿®æ”¹è¾“å‡ºè·¯å¾„\n        'augment_train': False  # ä¸ä½¿ç”¨æ•°æ®å¢å¼º\n    }\n    print(\"ğŸ§ª ä½¿ç”¨TESTæ¨¡å¼é…ç½® - é€‚åˆå¿«é€ŸéªŒè¯ä»£ç \")\n    \nelif RUNNING_MODE == \"TRAIN\":\n    # ğŸš€ TRAINæ¨¡å¼é…ç½® - å®Œæ•´è®­ç»ƒæµç¨‹\n    config = {\n        # æ•°æ®å‚æ•° - ä½¿ç”¨å®Œæ•´æ•°æ®é›†\n        'data_path': '../dataset',\n        'patch_size': 64,     # æ ‡å‡†patchå¤§å°\n        'stride': 32,         # æ ‡å‡†stride\n        'test_size': 0.2,\n        'val_size': 0.1,\n        'batch_size': 8,      # é€‚ä¸­çš„batch size\n        'num_workers': 4,\n        \n        # æ¨¡å‹å‚æ•° - æ ‡å‡†é…ç½®\n        'input_channels': 8,\n        'temporal_steps': 28,\n        'swin_patch_size': 4,\n        'embed_dim': 96,      # æ ‡å‡†åµŒå…¥ç»´åº¦\n        'depths': [2, 2, 6, 2],  # æ ‡å‡†æ·±åº¦\n        'num_heads': [3, 6, 12, 24],  # æ ‡å‡†æ³¨æ„åŠ›å¤´æ•°\n        'window_size': 7,     # æ ‡å‡†çª—å£å¤§å°\n        'mlp_ratio': 4.0,\n        'dropout': 0.0,\n        'attn_dropout': 0.0,\n        'drop_path_rate': 0.1,\n        \n        # è®­ç»ƒå‚æ•° - å®Œæ•´è®­ç»ƒ\n        'epochs': 100,        # å®Œæ•´è®­ç»ƒè½®æ•°\n        'learning_rate': 1e-4, # æ ‡å‡†å­¦ä¹ ç‡\n        'weight_decay': 0.05, # Swinæ¨èçš„æƒé‡è¡°å‡\n        'gradient_accumulation_steps': 2,\n        'max_grad_norm': 5.0,\n        \n        # æŸå¤±å‡½æ•°\n        'use_combined_loss': True,  # ä½¿ç”¨ç»„åˆæŸå¤±\n        'focal_gamma': 2.5,\n        'label_smoothing': 0.1,\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        'scheduler_type': 'cosine',\n        'min_lr': 1e-6,\n        \n        # å…¶ä»–\n        'patience': 15,       # é•¿è€å¿ƒå€¼\n        'save_dir': '../Models/swin-transformer/train',  # ä¿®æ”¹è¾“å‡ºè·¯å¾„\n        'augment_train': True  # ä½¿ç”¨æ•°æ®å¢å¼º\n    }\n    print(\"ğŸš€ ä½¿ç”¨TRAINæ¨¡å¼é…ç½® - é€‚åˆå®Œæ•´è®­ç»ƒ\")\n\n# åˆ›å»ºä¿å­˜ç›®å½•\nPath(config['save_dir']).mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nğŸ“‹ å½“å‰é…ç½® ({RUNNING_MODE}æ¨¡å¼):\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")\n\nif RUNNING_MODE == \"TEST\":\n    print(\"\\nâš ï¸ æ³¨æ„: TESTæ¨¡å¼ä½¿ç”¨ç®€åŒ–é…ç½®ï¼Œä»…ç”¨äºéªŒè¯ä»£ç æ­£ç¡®æ€§!\")\nelse:\n    print(\"\\nğŸ’ª TRAINæ¨¡å¼: ä½¿ç”¨å®Œæ•´é…ç½®è¿›è¡Œæœ€ä½³æ€§èƒ½è®­ç»ƒ!\")\n\nprint(f\"\\nğŸ’¾ æ¨¡å‹è¾“å‡ºç›®å½•: {config['save_dir']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”„ åŠ è½½å’Œå‡†å¤‡æ•°æ®ï¼ˆé€‚é…ä¸åŒæ¨¡å¼ï¼‰\nprint(f\\\"ğŸ”„ åŠ è½½æ•°æ® ({RUNNING_MODE}æ¨¡å¼)...\\\")\\n\\ntrain_loader, val_loader, test_loader, data_info = prepare_data(\\n    data_path=config['data_path'],\\n    patch_size=config['patch_size'],\\n    stride=config['stride'],\\n    test_size=config['test_size'],\\n    val_size=config['val_size'],\\n    batch_size=config['batch_size'],\\n    num_workers=config['num_workers'],\\n    augment_train=config['augment_train'],\\n    swin_patch_size=config['swin_patch_size']\\n)\\n\\nprint(f\\\"\\\\nğŸ“Š æ•°æ®é›†ä¿¡æ¯ ({RUNNING_MODE}æ¨¡å¼):\\\")\\nprint(f\\\"  ç±»åˆ«æ•°é‡: {data_info['num_classes']}\\\")\\nprint(f\\\"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}\\\")\\nprint(f\\\"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)}\\\")\\nprint(f\\\"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}\\\")\\nprint(f\\\"  è°ƒæ•´åpatchå¤§å°: {data_info['patch_size']}\\\")\\nprint(f\\\"  Swin patchå¤§å°: {data_info['swin_patch_size']}\\\")\\nprint(f\\\"  è¾“å…¥å½¢çŠ¶: {data_info['input_shape']}\\\")\\n\\n# ä¼°ç®—æ•°æ®é‡\\ntotal_train_samples = len(train_loader) * config['batch_size']\\ntotal_val_samples = len(val_loader) * config['batch_size']\\nprint(f\\\"  è®­ç»ƒæ ·æœ¬æ•°: ~{total_train_samples}\\\")\\nprint(f\\\"  éªŒè¯æ ·æœ¬æ•°: ~{total_val_samples}\\\")\\n\\nif RUNNING_MODE == \\\"TEST\\\":\\n    print(f\\\"\\\\nğŸ§ª TESTæ¨¡å¼æ•°æ®é‡: ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•\\\")\\n    print(f\\\"  é¢„è®¡å•epochæ—¶é—´: ~30ç§’\\\")\\n    print(f\\\"  æ€»è®­ç»ƒæ—¶é—´: ~3åˆ†é’Ÿ\\\")\\nelse:\\n    print(f\\\"\\\\nğŸš€ TRAINæ¨¡å¼æ•°æ®é‡: ä½¿ç”¨å®Œæ•´æ•°æ®é›†\\\")\\n    print(f\\\"  é¢„è®¡å•epochæ—¶é—´: ~5-10åˆ†é’Ÿ\\\")\\n    print(f\\\"  æ€»è®­ç»ƒæ—¶é—´: ~8-16å°æ—¶\\\")\\n\\n# ä¿å­˜æ•°æ®ä¿¡æ¯\\nsave_data_info(data_info, f\\\"{config['save_dir']}/data_info.pkl\\\")\\n\\n# æ˜¾ç¤ºç±»åˆ«ä¿¡æ¯\\nprint(f\\\"\\\\nğŸ·ï¸ ç±»åˆ«ä¿¡æ¯:\\\")\\nfor idx, name in data_info['class_names'].items():\\n    print(f\\\"  {idx}: {name}\\\")\\n\\nprint(f\\\"\\\\nâš–ï¸ ç±»åˆ«æƒé‡: {data_info['class_weights'].numpy()}\\\")\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š æ•°æ®å¯è§†åŒ–ï¼ˆé€‚é…ä¸åŒæ¨¡å¼ï¼‰\\n# è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–\\ndata_iter = iter(train_loader)\\nsample_batch_x, sample_batch_y = next(data_iter)\\n\\nprint(f\\\"æ ·æœ¬æ‰¹æ¬¡å½¢çŠ¶ - X: {sample_batch_x.shape}, Y: {sample_batch_y.shape}\\\")\\nprint(f\\\"Xæ•°å€¼èŒƒå›´: [{sample_batch_x.min():.3f}, {sample_batch_x.max():.3f}]\\\")\\nprint(f\\\"Yå”¯ä¸€å€¼: {torch.unique(sample_batch_y)}\\\")\\n\\n# æ ¹æ®æ¨¡å¼å†³å®šå¯è§†åŒ–æ ·æœ¬æ•°é‡\\nif RUNNING_MODE == \\\"TEST\\\":\\n    num_samples = min(4, sample_batch_x.shape[0])  # TESTæ¨¡å¼æ˜¾ç¤ºæ›´å°‘æ ·æœ¬\\n    print(f\\\"\\\\nğŸ§ª TESTæ¨¡å¼: æ˜¾ç¤º {num_samples} ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€ŸéªŒè¯\\\")\\nelse:\\n    num_samples = min(8, sample_batch_x.shape[0])  # TRAINæ¨¡å¼æ˜¾ç¤ºæ›´å¤šæ ·æœ¬\\n    print(f\\\"\\\\nğŸš€ TRAINæ¨¡å¼: æ˜¾ç¤º {num_samples} ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ\\\")\\n\\n# å¯è§†åŒ–RGBæ ·æœ¬\\nrows = 2 if num_samples > 4 else 1\\ncols = min(4, num_samples)\\nfig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\\nfig.suptitle(f'Swin Transformer Input Samples - RGB Visualization ({RUNNING_MODE} Mode)', fontsize=16)\\n\\nif rows == 1:\\n    axes = axes if num_samples > 1 else [axes]\\nelse:\\n    axes = axes.ravel()\\n\\nfor i in range(num_samples):\\n    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„RGBåˆæˆå›¾åƒ\\n    rgb_img = sample_batch_x[i, :, :, 0, :3].numpy()\\n    rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\\n    \\n    ax_idx = i if rows == 1 else i\\n    axes[ax_idx].imshow(rgb_img)\\n    axes[ax_idx].set_title(f'Sample {i+1} - RGB (t=0)')\\n    axes[ax_idx].axis('off')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# å¯è§†åŒ–å¯¹åº”çš„æ ‡ç­¾\\nfig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\\nfig.suptitle(f'Ground Truth Labels ({RUNNING_MODE} Mode)', fontsize=16)\\n\\nif rows == 1:\\n    axes = axes if num_samples > 1 else [axes]\\nelse:\\n    axes = axes.ravel()\\n\\nfor i in range(num_samples):\\n    ax_idx = i if rows == 1 else i\\n    im = axes[ax_idx].imshow(sample_batch_y[i].numpy(), cmap='tab10', vmin=0, vmax=7)\\n    axes[ax_idx].set_title(f'Sample {i+1} - Labels')\\n    axes[ax_idx].axis('off')\\n\\nplt.tight_layout()\\nplt.show()\\n\\nif RUNNING_MODE == \\\"TEST\\\":\\n    print(\\\"âœ“ æ•°æ®åŠ è½½æµ‹è¯•å®Œæˆï¼Œä»£ç è¿è¡Œæ­£å¸¸!\\\")\\nelse:\\n    print(\\\"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼Œå‡†å¤‡å¼€å§‹å®Œæ•´è®­ç»ƒ!\\\")\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹åˆ›å»ºå’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®¾å¤‡\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# åˆ›å»ºSwin Transformeræ¨¡å‹\n",
    "model = create_swin_model(\n",
    "    input_channels=config['input_channels'],\n",
    "    temporal_steps=config['temporal_steps'],\n",
    "    num_classes=data_info['num_classes'],\n",
    "    patch_size=config['swin_patch_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    depths=config['depths'],\n",
    "    num_heads=config['num_heads'],\n",
    "    window_size=config['window_size'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    drop_rate=config['dropout'],\n",
    "    attn_drop_rate=config['attn_dropout'],\n",
    "    drop_path_rate=config['drop_path_rate']\n",
    ").to(device)\n",
    "\n",
    "# è®¡ç®—å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nğŸ”§ Swin Transformeræ¨¡å‹ä¿¡æ¯:\")\n",
    "print(f\"  æ€»å‚æ•°é‡: {total_params:,}\")\n",
    "print(f\"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "print(f\"  æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.1f} MB (FP32)\")\n",
    "print(f\"  åµŒå…¥ç»´åº¦: {config['embed_dim']}\")\n",
    "print(f\"  å±‚æ•°é…ç½®: {config['depths']}\")\n",
    "print(f\"  æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}\")\n",
    "print(f\"  çª—å£å¤§å°: {config['window_size']}\")\n",
    "\n",
    "# æµ‹è¯•å‰å‘ä¼ æ’­\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = sample_batch_x[:2].to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"\\nğŸ§ª å‰å‘ä¼ æ’­æµ‹è¯•:\")\n",
    "    print(f\"  è¾“å…¥å½¢çŠ¶: {test_input.shape}\")\n",
    "    print(f\"  è¾“å‡ºå½¢çŠ¶: {test_output.shape}\")\n",
    "    print(f\"  è¾“å‡ºæ•°å€¼èŒƒå›´: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "\n",
    "print(\"\\nâœ… Swin Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®æŸå¤±å‡½æ•°\n",
    "if config['use_combined_loss']:\n",
    "    criterion = CombinedLoss(\n",
    "        alpha=data_info['class_weights'].to(device),\n",
    "        gamma=config['focal_gamma'],\n",
    "        label_smoothing=config['label_smoothing']\n",
    "    )\n",
    "    print(\"ğŸ¯ ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•° (Focal + Dice + CrossEntropy)\")\nelse:\n    criterion = nn.CrossEntropyLoss(\n",
    "        weight=data_info['class_weights'].to(device),\n",
    "        label_smoothing=config['label_smoothing']\n",
    "    )\n    print(\"ğŸ¯ ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°\")\n",
    "\n",
    "# é…ç½®ä¼˜åŒ–å™¨ - Swin Transformerä¸“ç”¨å‚æ•°\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "print(f\"âš™ï¸ ä¼˜åŒ–å™¨: AdamW (lr={config['learning_rate']}, wd={config['weight_decay']})\")\n",
    "\n",
    "# é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "if config['scheduler_type'] == 'cosine':\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,\n",
    "        T_mult=2,\n",
    "        eta_min=config['min_lr']\n",
    "    )\n",
    "    print(f\"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨: CosineAnnealingWarmRestarts (min_lr={config['min_lr']})\")\nelse:\n    scheduler = None\n    print(\"ğŸ“ˆ æ— å­¦ä¹ ç‡è°ƒåº¦å™¨\")\n",
    "\n",
    "# æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler()\n",
    "print(\"âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\")\n",
    "\n",
    "# æ—©åœ\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config['patience'],\n",
    "    verbose=True,\n",
    "    save_path=f\"{config['save_dir']}/best_model.pth\"\n",
    ")\n",
    "print(f\"â¹ï¸ æ—©åœæœºåˆ¶ (patience={config['patience']})\")\n",
    "\n",
    "print(\"\\nâœ… è®­ç»ƒç»„ä»¶é…ç½®å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸš€ è®­ç»ƒå‡†å¤‡ï¼ˆé€‚é…ä¸åŒæ¨¡å¼ï¼‰\\nhistory = {\\n    'train_loss': [],\\n    'train_acc': [],\\n    'val_loss': [],\\n    'val_acc': [],\\n    'val_miou': [],\\n    'learning_rates': []\\n}\\n\\nbest_miou = 0.0\\nstart_time = time.time()\\n\\nif RUNNING_MODE == \\\"TEST\\\":\\n    print(f\\\"ğŸ§ª å¼€å§‹TESTæ¨¡å¼è®­ç»ƒ - Swin Transformer ({config['epochs']} epochs)\\\")\\n    print(f\\\"âš¡ ç›®æ ‡: å¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\\\")\\n    print(f\\\"ğŸ”§ æ¨¡å‹é…ç½®: è½»é‡çº§ (embed_dim={config['embed_dim']}, depths={config['depths']})\\\")\\nelse:\\n    print(f\\\"ğŸš€ å¼€å§‹TRAINæ¨¡å¼è®­ç»ƒ - Swin Transformer ({config['epochs']} epochs)\\\")\\n    print(f\\\"ğŸ¯ ç›®æ ‡: è·å¾—æœ€ä½³æ¨¡å‹æ€§èƒ½\\\")\\n    print(f\\\"ğŸ’ª æ¨¡å‹é…ç½®: æ ‡å‡†é…ç½® (embed_dim={config['embed_dim']}, depths={config['depths']})\\\")\\n\\nprint(f\\\"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\")\\nprint(f\\\"ğŸ¯ ç‰¹å¾: {config['depths']}å±‚æ·±åº¦, {config['embed_dim']}åµŒå…¥ç»´åº¦, {config['window_size']}çª—å£å¤§å°\\\")\\nprint(\\\"=\\\" * 80)\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{config['epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # è®­ç»ƒé˜¶æ®µ\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, scaler, epoch,\n",
    "        scheduler=None,  # æˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨scheduler\n",
    "        scheduler_step_per_batch=False,\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        max_grad_norm=config['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    # éªŒè¯é˜¶æ®µ\n",
    "    val_loss, val_acc, val_metrics = validate(\n",
    "        model, val_loader, criterion, device, data_info['num_classes']\n",
    "    )\n",
    "    \n",
    "    # å­¦ä¹ ç‡è°ƒåº¦\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # è·å–å½“å‰å­¦ä¹ ç‡\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # è®°å½•å†å²\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_miou'].append(val_metrics['mean_iou'])\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # è®¡ç®—epochæ—¶é—´\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(f\"Epoch {epoch} ç»“æœ:\")\n",
    "    print(f\"  è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  éªŒè¯ mIoU: {val_metrics['mean_iou']:.4f}\")\n",
    "    print(f\"  å­¦ä¹ ç‡: {current_lr:.2e}\")\n",
    "    print(f\"  è€—æ—¶: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    if val_metrics['mean_iou'] > best_miou:\n",
    "        best_miou = val_metrics['mean_iou']\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_miou': best_miou,\n",
    "            'config': config,\n",
    "            'metrics': val_metrics\n",
    "        }, f\"{config['save_dir']}/best_model.pth\")\n",
    "        print(f\"  ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! (mIoU: {best_miou:.4f})\")\n",
    "    \n",
    "    # æ—©åœæ£€æŸ¥\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"\\nâ¹ï¸ è§¦å‘æ—©åœ!\")\n",
    "        break\n",
    "    \n",
    "    # æ¯5ä¸ªepochæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦\n",
    "    if epoch % 5 == 0:\n",
    "        total_time = time.time() - start_time\n",
    "        avg_epoch_time = total_time / epoch\n",
    "        estimated_total = avg_epoch_time * config['epochs']\n",
    "        print(f\"  ğŸ“Š è¿›åº¦: {epoch/config['epochs']*100:.1f}%, é¢„è®¡å‰©ä½™æ—¶é—´: {(estimated_total - total_time)/60:.1f}åˆ†é’Ÿ\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nğŸ‰ Swin Transformerè®­ç»ƒå®Œæˆ!\")\n",
    "print(f\"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ\")\n",
    "print(f\"ğŸ† æœ€ä½³ mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜è®­ç»ƒå†å²\n",
    "with open(f\"{config['save_dir']}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(\"ğŸ’¾ è®­ç»ƒå†å²å·²ä¿å­˜\")\n",
    "\n",
    "# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "plot_training_history(\n",
    "    history, \n",
    "    save_path=f\"{config['save_dir']}/training_curves.png\",\n",
    "    title=\"Swin Transformer Training History\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°\n",
    "print(\"ğŸ“‚ åŠ è½½æœ€ä½³Swin Transformeræ¨¡å‹è¿›è¡Œè¯„ä¼°...\")\n",
    "\n",
    "# é‡æ–°åˆ›å»ºæ¨¡å‹\n",
    "eval_model = create_swin_model(\n",
    "    input_channels=config['input_channels'],\n",
    "    temporal_steps=config['temporal_steps'],\n",
    "    num_classes=data_info['num_classes'],\n",
    "    patch_size=config['swin_patch_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    depths=config['depths'],\n",
    "    num_heads=config['num_heads'],\n",
    "    window_size=config['window_size'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    drop_rate=config['dropout'],\n",
    "    attn_drop_rate=config['attn_dropout'],\n",
    "    drop_path_rate=0.0  # è¯„ä¼°æ—¶ä¸ä½¿ç”¨drop_path\n",
    ").to(device)\n",
    "\n",
    "# åŠ è½½æœ€ä½³æƒé‡\n",
    "checkpoint = load_checkpoint(f\"{config['save_dir']}/best_model.pth\", eval_model)\n",
    "eval_model.eval()\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒepoch: {checkpoint['epoch']})\")\n",
    "print(f\"ğŸ† è®­ç»ƒæ—¶æœ€ä½³mIoU: {checkpoint.get('best_miou', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "print(\"ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°Swin Transformeræ¨¡å‹...\")\n",
    "\n",
    "eval_model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets_cpu = targets.numpy()\n",
    "        \n",
    "        outputs = eval_model(data)  # (batch, height, width, num_classes)\n",
    "        probs = torch.softmax(outputs, dim=-1)\n",
    "        _, predicted = outputs.max(-1)\n",
    "        \n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_targets.append(targets_cpu)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡\n",
    "all_preds = np.concatenate(all_preds).flatten()\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "\n",
    "# è®¡ç®—è¯¦ç»†æŒ‡æ ‡\n",
    "metrics = calculate_metrics(all_preds, all_targets, data_info['num_classes'])\n",
    "metrics['class_names'] = data_info['class_names']\n",
    "\n",
    "print(f\"\\nğŸ“Š Swin Transformeræµ‹è¯•é›†è¯„ä¼°ç»“æœ:\")\n",
    "print(f\"  æ€»ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}\")\n",
    "print(f\"  å¹³å‡å‡†ç¡®ç‡: {metrics['mean_accuracy']:.4f}\")\n",
    "print(f\"  å¹³å‡IoU: {metrics['mean_iou']:.4f}\")\n",
    "print(f\"  å¹³å‡ç²¾ç¡®ç‡: {metrics['mean_precision']:.4f}\")\n",
    "print(f\"  å¹³å‡å¬å›ç‡: {metrics['mean_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡\n",
    "print(f\"\\nğŸ“‹ å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ (Swin Transformer):\")\n",
    "print(f\"{'ç±»åˆ«':<15} {'å‡†ç¡®ç‡':<10} {'IoU':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(data_info['num_classes']):\n",
    "    class_name = data_info['class_names'][i]\n",
    "    acc = metrics['per_class_accuracy'][i]\n",
    "    iou = metrics['per_class_iou'][i]\n",
    "    precision = metrics['per_class_precision'][i]\n",
    "    recall = metrics['per_class_recall'][i]\n",
    "    \n",
    "    print(f\"{class_name:<15} {acc:<10.4f} {iou:<10.4f} {precision:<10.4f} {recall:<10.4f}\")\n",
    "\n",
    "# æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«\n",
    "best_class_idx = np.argmax(metrics['per_class_iou'])\n",
    "worst_class_idx = np.argmin(metrics['per_class_iou'])\n",
    "\n",
    "print(f\"\\nğŸ¥‡ è¡¨ç°æœ€å¥½çš„ç±»åˆ«: {data_info['class_names'][best_class_idx]} (IoU: {metrics['per_class_iou'][best_class_idx]:.4f})\")\n",
    "print(f\"ğŸ¥‰ è¡¨ç°æœ€å·®çš„ç±»åˆ«: {data_info['class_names'][worst_class_idx]} (IoU: {metrics['per_class_iou'][worst_class_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç»“æœå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "plot_confusion_matrix(\n",
    "    metrics['confusion_matrix'],\n",
    "    list(data_info['class_names'].values()),\n",
    "    save_path=f\"{config['save_dir']}/confusion_matrix.png\",\n",
    "    title=\"Swin Transformer Confusion Matrix\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å›¾è¡¨\n",
    "plot_class_performance(\n",
    "    metrics, \n",
    "    save_path=f\"{config['save_dir']}/class_performance.png\",\n",
    "    title=\"Swin Transformer Per-Class Performance\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ˆ ç±»åˆ«æ€§èƒ½å›¾è¡¨å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–é¢„æµ‹ç»“æœ\n",
    "print(\"ğŸ¨ ç”ŸæˆSwin Transformeré¢„æµ‹å¯è§†åŒ–...\")\n",
    "\n",
    "# è·å–ä¸€æ‰¹æµ‹è¯•æ•°æ®\n",
    "test_iter = iter(test_loader)\n",
    "test_batch_x, test_batch_y = next(test_iter)\n",
    "\n",
    "# è¿›è¡Œé¢„æµ‹\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_x_device = test_batch_x.to(device)\n",
    "    test_outputs = eval_model(test_batch_x_device)\n",
    "    _, test_predictions = test_outputs.max(-1)\n",
    "\n",
    "# å¯è§†åŒ–é¢„æµ‹ç»“æœ\n",
    "visualize_predictions(\n",
    "    test_batch_x[:6],  # æ˜¾ç¤º6ä¸ªæ ·æœ¬\n",
    "    test_batch_y[:6],\n",
    "    test_predictions[:6].cpu(),\n",
    "    list(data_info['class_names'].values()),\n",
    "    num_samples=6,\n",
    "    save_path=f\"{config['save_dir']}/predictions_visualization.png\",\n",
    "    title=\"Swin Transformer Predictions vs Ground Truth\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ–¼ï¸ é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. çª—å£æ³¨æ„åŠ›åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swin Transformerç‰¹æœ‰çš„çª—å£æ³¨æ„åŠ›åˆ†æ\n",
    "print(\"ğŸ” åˆ†æSwin Transformerçš„çª—å£æ³¨æ„åŠ›æ¨¡å¼...\")\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ\n",
    "sample_idx = 0\n",
    "sample_input = test_batch_x[sample_idx:sample_idx+1].to(device)\n",
    "sample_target = test_batch_y[sample_idx]\n",
    "\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    # è·å–é¢„æµ‹\n",
    "    sample_output = eval_model(sample_input)\n",
    "    sample_probs = torch.softmax(sample_output, dim=-1)\n",
    "    confidence, prediction = sample_probs.max(-1)\n",
    "    \n",
    "    # å°è¯•è·å–patch embeddingç‰¹å¾\n",
    "    try:\n",
    "        patch_features, (patch_h, patch_w) = eval_model.patch_embed(sample_input)\n",
    "        print(f\"âœ“ Patch features shape: {patch_features.shape}\")\n",
    "        print(f\"âœ“ Patch resolution: {patch_h} x {patch_w}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Cannot extract patch features: {e}\")\n",
    "        patch_features = None\n",
    "\n",
    "# åˆ›å»ºçª—å£åˆ†æå¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "fig.suptitle('Swin Transformer Window Analysis', fontsize=16)\n",
    "\n",
    "# åŸå§‹RGBå›¾åƒ\n",
    "rgb_img = sample_input[0, :, :, 0, :3].cpu().numpy()\n",
    "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "axes[0, 0].imshow(rgb_img)\n",
    "axes[0, 0].set_title('Input RGB (t=0)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# çœŸå®æ ‡ç­¾\n",
    "axes[0, 1].imshow(sample_target.numpy(), cmap='tab10')\n",
    "axes[0, 1].set_title('Ground Truth')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# é¢„æµ‹ç»“æœ\n",
    "pred_img = prediction[0].cpu().numpy()\n",
    "axes[0, 2].imshow(pred_img, cmap='tab10')\n",
    "axes[0, 2].set_title('Swin Prediction')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# é¢„æµ‹ç½®ä¿¡åº¦\n",
    "conf_img = confidence[0].cpu().numpy()\n",
    "im1 = axes[0, 3].imshow(conf_img, cmap='viridis')\n",
    "axes[0, 3].set_title('Prediction Confidence')\n",
    "axes[0, 3].axis('off')\n",
    "plt.colorbar(im1, ax=axes[0, 3])\n",
    "\n",
    "# çª—å£åˆ’åˆ†å¯è§†åŒ–\n",
    "window_size = config['window_size']\n",
    "H, W = sample_input.shape[1:3]\n",
    "\n",
    "# åˆ›å»ºçª—å£ç½‘æ ¼\n",
    "window_grid = np.zeros((H, W))\n",
    "window_id = 0\n",
    "for i in range(0, H, window_size):\n",
    "    for j in range(0, W, window_size):\n",
    "        end_i = min(i + window_size, H)\n",
    "        end_j = min(j + window_size, W)\n",
    "        window_grid[i:end_i, j:end_j] = window_id % 8  # 8ç§é¢œè‰²å¾ªç¯\n",
    "        window_id += 1\n",
    "\n",
    "im2 = axes[1, 0].imshow(window_grid, cmap='Set3')\n",
    "axes[1, 0].set_title(f'Window Partition ({window_size}x{window_size})')\n",
    "axes[1, 0].axis('off')\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# patchç‰¹å¾å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "if patch_features is not None and patch_features.shape[1] > 0:\n",
    "    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç‰¹å¾é€šé“\n",
    "    feature_map = patch_features[0, :, 0].view(patch_h, patch_w).cpu().numpy()\n",
    "    im3 = axes[1, 1].imshow(feature_map, cmap='coolwarm')\n",
    "    axes[1, 1].set_title('Patch Features (Channel 0)')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[1, 1])\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Patch Features\\nNot Available', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "# è®¡ç®—æ ·æœ¬å‡†ç¡®ç‡\n",
    "sample_accuracy = (pred_img == sample_target.numpy()).mean()\n",
    "\n",
    "# ç±»åˆ«åˆ†å¸ƒ\n",
    "unique, counts = np.unique(pred_img, return_counts=True)\n",
    "axes[1, 2].bar(unique, counts, color='lightblue', edgecolor='navy')\n",
    "axes[1, 2].set_xlabel('Class')\n",
    "axes[1, 2].set_ylabel('Pixel Count')\n",
    "axes[1, 2].set_title('Predicted Class Distribution')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# æ€§èƒ½æ‘˜è¦\n",
    "axes[1, 3].text(0.1, 0.8, f'Sample Accuracy: {sample_accuracy:.3f}', transform=axes[1, 3].transAxes, fontsize=12)\n",
    "axes[1, 3].text(0.1, 0.7, f'Mean Confidence: {conf_img.mean():.3f}', transform=axes[1, 3].transAxes, fontsize=12)\n",
    "axes[1, 3].text(0.1, 0.6, f'Window Size: {window_size}x{window_size}', transform=axes[1, 3].transAxes, fontsize=12)\n",
    "axes[1, 3].text(0.1, 0.5, f'Embed Dim: {config[\"embed_dim\"]}', transform=axes[1, 3].transAxes, fontsize=12)\n",
    "axes[1, 3].text(0.1, 0.4, f'Depths: {config[\"depths\"]}', transform=axes[1, 3].transAxes, fontsize=12)\n",
    "axes[1, 3].set_title('Model Configuration')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['save_dir']}/window_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ” çª—å£æ³¨æ„åŠ›åˆ†æå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºå•ä¸ªpatchçš„æ¨ç†\n",
    "print(\"ğŸ”® æ¼”ç¤ºSwin Transformeræ¨¡å‹æ¨ç†...\")\n",
    "\n",
    "# è·å–ä¸€ä¸ªæ ·æœ¬\n",
    "sample_x = test_batch_x[0:1].to(device)  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "sample_y = test_batch_y[0:1]\n",
    "\n",
    "# æ¨ç†\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    inference_start = time.time()\n",
    "    outputs = eval_model(sample_x)\n",
    "    inference_time = time.time() - inference_start\n",
    "    \n",
    "    # è·å–é¢„æµ‹å’Œç½®ä¿¡åº¦\n",
    "    probs = torch.softmax(outputs, dim=-1)\n",
    "    confidence, predictions = probs.max(-1)\n",
    "    \n",
    "    # è®¡ç®—å‡†ç¡®ç‡\n",
    "    accuracy = (predictions.cpu() == sample_y).float().mean().item()\n",
    "\n",
    "print(f\"\\nâš¡ Swin Transformeræ¨ç†æ€§èƒ½:\")\n",
    "print(f\"  æ¨ç†æ—¶é—´: {inference_time*1000:.2f}ms\")\n",
    "print(f\"  è¾“å…¥å½¢çŠ¶: {sample_x.shape}\")\n",
    "print(f\"  è¾“å‡ºå½¢çŠ¶: {outputs.shape}\")\n",
    "print(f\"  é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "print(f\"  å¹³å‡ç½®ä¿¡åº¦: {confidence.mean():.4f}\")\n",
    "print(f\"  æœ€å°ç½®ä¿¡åº¦: {confidence.min():.4f}\")\n",
    "print(f\"  æœ€å¤§ç½®ä¿¡åº¦: {confidence.max():.4f}\")\n",
    "print(f\"  çª—å£å¤§å°: {config['window_size']}x{config['window_size']}\")\n",
    "print(f\"  å±‚æ¬¡ç»“æ„: {config['depths']} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å•ä¸ªæ ·æœ¬çš„æ¨ç†ç»“æœå’Œçª—å£åˆ†æ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Swin Transformer Single Sample Analysis', fontsize=16)\n",
    "\n",
    "# åŸå§‹RGBå›¾åƒ\n",
    "rgb_img = sample_x[0, :, :, 0, :3].cpu().numpy()\n",
    "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "axes[0, 0].imshow(rgb_img)\n",
    "axes[0, 0].set_title('Input RGB (t=0)', fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# çœŸå®æ ‡ç­¾\n",
    "gt_img = sample_y[0].numpy()\n",
    "im1 = axes[0, 1].imshow(gt_img, cmap='tab10', vmin=0, vmax=7)\n",
    "axes[0, 1].set_title('Ground Truth', fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# é¢„æµ‹ç»“æœ\n",
    "pred_img = predictions[0].cpu().numpy()\n",
    "im2 = axes[0, 2].imshow(pred_img, cmap='tab10', vmin=0, vmax=7)\n",
    "axes[0, 2].set_title(f'Swin Prediction (Acc: {accuracy:.3f})', fontsize=14)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# ç½®ä¿¡åº¦å›¾\n",
    "conf_img = confidence[0].cpu().numpy()\n",
    "im3 = axes[1, 0].imshow(conf_img, cmap='viridis')\n",
    "axes[1, 0].set_title('Prediction Confidence', fontsize=14)\n",
    "axes[1, 0].axis('off')\n",
    "plt.colorbar(im3, ax=axes[1, 0], label='Confidence')\n",
    "\n",
    "# è¯¯å·®å›¾ï¼ˆçº¢è‰²è¡¨ç¤ºé”™è¯¯é¢„æµ‹ï¼‰\n",
    "error_map = (pred_img != gt_img).astype(float)\n",
    "im4 = axes[1, 1].imshow(error_map, cmap='Reds')\n",
    "axes[1, 1].set_title(f'Prediction Errors ({error_map.mean()*100:.1f}%)', fontsize=14)\n",
    "axes[1, 1].axis('off')\n",
    "plt.colorbar(im4, ax=axes[1, 1], label='Error')\n",
    "\n",
    "# ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾\n",
    "axes[1, 2].hist(conf_img.flatten(), bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 2].axvline(conf_img.mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean: {conf_img.mean():.3f}')\n",
    "axes[1, 2].set_xlabel('Confidence')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Confidence Distribution')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['save_dir']}/single_sample_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¨ å•æ ·æœ¬Swin Transformeræ¨ç†å¯è§†åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### ğŸ¯ Swin Transformerè®­ç»ƒç»“æœæ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\\\"=\\\" * 80)\\nprint(f\\\"ğŸ¯ SWIN TRANSFORMER å†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒæ€»ç»“ ({RUNNING_MODE}æ¨¡å¼)\\\")\\nprint(\\\"=\\\" * 80)\\n\\nprint(f\\\"\\\\nğŸ”„ è¿è¡Œæ¨¡å¼: {RUNNING_MODE}\\\")\\nif RUNNING_MODE == \\\"TEST\\\":\\n    print(\\\"  âœ“ å¿«é€ŸéªŒè¯ä»£ç æ­£ç¡®æ€§\\\")\\n    print(\\\"  âœ“ ä½¿ç”¨è½»é‡çº§æ¨¡å‹é…ç½®\\\")\\n    print(\\\"  âœ“ å°‘é‡è®­ç»ƒè½®æ•°\\\")\\n    print(\\\"  âœ“ é€‚åˆæœ¬åœ°å¼€å‘ç¯å¢ƒ\\\")\\nelse:\\n    print(\\\"  âœ“ å®Œæ•´è®­ç»ƒæµç¨‹\\\")\\n    print(\\\"  âœ“ æ ‡å‡†æ¨¡å‹é…ç½®\\\")\\n    print(\\\"  âœ“ å……è¶³è®­ç»ƒè½®æ•°\\\")\\n    print(\\\"  âœ“ é€‚åˆäº‘ç«¯GPUç¯å¢ƒ\\\")\\n\\nprint(f\\\"\\\\nğŸ“Š æ¨¡å‹é…ç½®:\\\")\\nprint(f\\\"  æ¨¡å‹ç±»å‹: Swin Transformer\\\")\\nprint(f\\\"  åµŒå…¥ç»´åº¦: {config['embed_dim']}\\\")\\nprint(f\\\"  å±‚æ•°é…ç½®: {config['depths']}\\\")\\nprint(f\\\"  æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}\\\")\\nprint(f\\\"  çª—å£å¤§å°: {config['window_size']}\\\")\\nprint(f\\\"  æ€»å‚æ•°é‡: {total_params:,}\\\")\\n\\nprint(f\\\"\\\\nğŸƒ è®­ç»ƒè¿‡ç¨‹:\\\")\\nprint(f\\\"  è®­ç»ƒepochs: {len(history['train_loss'])}\\\")\\nprint(f\\\"  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ\\\")\\nprint(f\\\"  å¹³å‡æ¯epoch: {total_training_time/len(history['train_loss']):.1f}ç§’\\\")\\nprint(f\\\"  æœ€ä½³éªŒè¯mIoU: {best_miou:.4f}\\\")\\nif len(history['learning_rates']) > 0:\\n    print(f\\\"  æœ€ç»ˆå­¦ä¹ ç‡: {history['learning_rates'][-1]:.2e}\\\")\\n\\nprint(f\\\"\\\\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½:\\\")\\nprint(f\\\"  æµ‹è¯•é›†æ€»ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}\\\")\\nprint(f\\\"  æµ‹è¯•é›†å¹³å‡IoU: {metrics['mean_iou']:.4f}\\\")\\nprint(f\\\"  æµ‹è¯•é›†å¹³å‡ç²¾ç¡®ç‡: {metrics['mean_precision']:.4f}\\\")\\nprint(f\\\"  æµ‹è¯•é›†å¹³å‡å¬å›ç‡: {metrics['mean_recall']:.4f}\\\")\\n\\nprint(f\\\"\\\\nğŸ¯ Swin Transformerç‰¹è‰²:\\\")\\nprint(f\\\"  âœ“ å±‚æ¬¡åŒ–ç‰¹å¾æå–: 4ä¸ªé˜¶æ®µï¼Œåˆ†è¾¨ç‡é€æ­¥é™ä½\\\")\\nprint(f\\\"  âœ“ ç§»ä½çª—å£æ³¨æ„åŠ›: å»ºç«‹è·¨çª—å£è¿æ¥\\\")\\nprint(f\\\"  âœ“ çº¿æ€§å¤æ‚åº¦: ç›¸å¯¹äºè¾“å…¥å¤§å°å‘ˆçº¿æ€§å…³ç³»\\\")\\nprint(f\\\"  âœ“ å¤šå°ºåº¦èåˆ: é€šè¿‡Patch Mergingå®ç°ç‰¹å¾èåˆ\\\")\\n\\nprint(f\\\"\\\\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:\\\")\\nsave_dir = Path(config['save_dir'])\\nsaved_files = list(save_dir.glob('*'))\\nfor file in saved_files:\\n    print(f\\\"  {file.name}\\\")\\n\\nprint(f\\\"\\\\nğŸš€ ä½¿ç”¨å»ºè®®:\\\")\\nif RUNNING_MODE == \\\"TEST\\\":\\n    print(f\\\"  âœ… ä»£ç æµ‹è¯•å®Œæˆï¼Œå¯ä»¥åˆ‡æ¢åˆ°TRAINæ¨¡å¼è¿›è¡Œå®Œæ•´è®­ç»ƒ\\\")\\n    print(f\\\"  ğŸ“ ä¿®æ”¹ç¬¬ä¸€ä¸ªcell: RUNNING_MODE = 'TRAIN'\\\")\\n    print(f\\\"  ğŸ”„ é‡æ–°è¿è¡Œnotebookè¿›è¡Œå®Œæ•´è®­ç»ƒ\\\")\\n    print(f\\\"  â˜ï¸ å»ºè®®åœ¨äº‘ç«¯GPUä¸Šè¿è¡ŒTRAINæ¨¡å¼\\\")\\nelse:\\n    print(f\\\"  1. æ¨¡å‹å·²ä¿å­˜åˆ°: {config['save_dir']}/best_model.pth\\\")\\n    print(f\\\"  2. ç”¨äºæ¨ç†: python -m Swin-Transformer.inference --model-path {config['save_dir']}/best_model.pth\\\")\\n    print(f\\\"  3. çª—å£åˆ†æ: python -m Swin-Transformer.evaluate --analyze-windows\\\")\\n    print(f\\\"  4. æ¨¡å‹éƒ¨ç½²: è€ƒè™‘å¯¼å‡ºä¸ºONNXæ ¼å¼\\\")\\n\\nprint(f\\\"\\\\nğŸ“Š ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”:\\\")\\nprint(f\\\"  vs TCN: æ›´å¼ºçš„å¤šå°ºåº¦ç‰¹å¾æå–èƒ½åŠ›\\\")\\nprint(f\\\"  vs Vision Transformer: æ›´é«˜çš„è®¡ç®—æ•ˆç‡å’Œæ›´å¥½çš„å½’çº³åç½®\\\")\\nprint(f\\\"  ç‰¹ç‚¹: ç»“åˆCNNçš„å±‚æ¬¡åŒ–è®¾è®¡å’ŒTransformerçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\nif RUNNING_MODE == \\\"TEST\\\":\\n    print(\\\"ğŸ§ª Swin Transformerä»£ç æµ‹è¯•å®Œæˆ!\\\")\\n    print(\\\"âœ… æ‰€æœ‰ç»„ä»¶è¿è¡Œæ­£å¸¸ï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´è®­ç»ƒ!\\\")\\nelse:\\n    print(\\\"ğŸ‰ Swin Transformerå†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒå®Œæˆ!\\\")\\n    print(\\\"ğŸŒŸ å±‚æ¬¡åŒ–çª—å£æ³¨æ„åŠ›ï¼Œçº¿æ€§å¤æ‚åº¦ï¼ŒSOTAæ€§èƒ½!\\\")\\nprint(\\\"=\\\" * 80)\""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}