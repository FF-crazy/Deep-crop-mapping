{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Benchmark Evaluation\n",
    "\n",
    "This notebook evaluates all trained models on the same test dataset and compares their performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data preparation utilities\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "from TCN.dataset import prepare_data\n",
    "\n",
    "# Prepare data loaders with consistent parameters\n",
    "train_loader, val_loader, test_loader, data_info = prepare_data(\n",
    "    data_path=\"./dataset\",\n",
    "    patch_size=64,\n",
    "    stride=32,\n",
    "    test_size=0.2,\n",
    "    val_size=0.1,\n",
    "    random_state=42,  # Fixed seed for reproducibility\n",
    "    batch_size=16,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Class names mapping\n",
    "class_names = {\n",
    "    0: 'Corn',\n",
    "    1: 'Wheat', \n",
    "    2: 'Sunflower',\n",
    "    3: 'Pumpkin',\n",
    "    4: 'Artificial_Surface',\n",
    "    5: 'Water',\n",
    "    6: 'Road',\n",
    "    7: 'Other'\n",
    "}\n",
    "\n",
    "num_classes = data_info['num_classes']\n",
    "print(f\"Number of test samples: {len(test_loader.dataset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Test batch size: {test_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculate IoU for each class\"\"\"\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = pred == cls\n",
    "        target_cls = target == cls\n",
    "        intersection = (pred_cls & target_cls).sum()\n",
    "        union = (pred_cls | target_cls).sum()\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        ious.append(iou)\n",
    "    return np.array(ious)\n",
    "\n",
    "def evaluate_model(model, test_loader, device, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the test dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(test_loader, desc=f'Testing {model_name}'):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Handle different output shapes\n",
    "            if len(outputs.shape) == 4:  # (batch, height, width, classes)\n",
    "                _, predicted = outputs.max(-1)\n",
    "            elif len(outputs.shape) == 3:  # (batch, classes, height*width)\n",
    "                outputs = outputs.view(data.size(0), num_classes, data.size(2), data.size(3))\n",
    "                _, predicted = outputs.max(1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output shape: {outputs.shape}\")\n",
    "            \n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_targets.append(targets.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Flatten for metrics calculation\n",
    "    all_preds_flat = all_preds.flatten()\n",
    "    all_targets_flat = all_targets.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overall_acc = accuracy_score(all_targets_flat, all_preds_flat)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_acc = []\n",
    "    per_class_iou = []\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        mask = all_targets_flat == cls\n",
    "        if mask.sum() > 0:\n",
    "            class_acc = accuracy_score(all_targets_flat[mask], all_preds_flat[mask])\n",
    "            per_class_acc.append(class_acc)\n",
    "        else:\n",
    "            per_class_acc.append(0)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    ious = calculate_iou(all_preds_flat, all_targets_flat, num_classes)\n",
    "    per_class_iou = ious\n",
    "    \n",
    "    # F1 scores\n",
    "    f1_micro = f1_score(all_targets_flat, all_preds_flat, average='micro')\n",
    "    f1_macro = f1_score(all_targets_flat, all_preds_flat, average='macro')\n",
    "    f1_weighted = f1_score(all_targets_flat, all_preds_flat, average='weighted')\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_targets_flat, all_preds_flat, labels=range(num_classes))\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'mean_accuracy': np.mean(per_class_acc),\n",
    "        'mean_iou': np.mean(per_class_iou),\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'per_class_iou': per_class_iou,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds_flat,\n",
    "        'targets': all_targets_flat\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all results\n",
    "benchmark_results = {}\n",
    "\n",
    "# Get input shape from data\n",
    "sample_batch, _ = next(iter(test_loader))\n",
    "batch_size, height, width, temporal, spectral = sample_batch.shape\n",
    "print(f\"Input shape: {sample_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluate TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TCN model\n",
    "try:\n",
    "    from TCN.model import create_tcn_model\n",
    "    from TCN.compat import load_checkpoint\n",
    "    \n",
    "    # Create model\n",
    "    tcn_model = create_tcn_model(\n",
    "        input_shape=(height, width, temporal, spectral),\n",
    "        num_classes=num_classes,\n",
    "        hidden_channels=64,\n",
    "        kernel_size=3,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = './TCN/checkpoints_test/best_model.pth'\n",
    "    if Path(checkpoint_path).exists():\n",
    "        checkpoint = load_checkpoint(checkpoint_path, map_location=device)\n",
    "        tcn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"TCN model loaded from {checkpoint_path}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        tcn_results = evaluate_model(tcn_model, test_loader, device, \"TCN\")\n",
    "        benchmark_results['TCN'] = tcn_results\n",
    "    else:\n",
    "        print(f\"TCN checkpoint not found at {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading TCN model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Transformer model\n",
    "try:\n",
    "    from Transformer.model import create_transformer_model\n",
    "    \n",
    "    # Create model\n",
    "    transformer_model = create_transformer_model(\n",
    "        input_shape=(height, width, temporal, spectral),\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=128,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = './Transformer/checkpoints/best_model.pth'\n",
    "    if Path(checkpoint_path).exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        transformer_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Transformer model loaded from {checkpoint_path}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        transformer_results = evaluate_model(transformer_model, test_loader, device, \"Transformer\")\n",
    "        benchmark_results['Transformer'] = transformer_results\n",
    "    else:\n",
    "        print(f\"Transformer checkpoint not found at {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Transformer model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate Swin Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Swin Transformer model\n",
    "try:\n",
    "    from Swin_Transformer.model import create_swin_model\n",
    "    \n",
    "    # Create model\n",
    "    swin_model = create_swin_model(\n",
    "        input_shape=(height, width, temporal, spectral),\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = './Swin-Transformer/checkpoints/best_model.pth'\n",
    "    if Path(checkpoint_path).exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        swin_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Swin Transformer model loaded from {checkpoint_path}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        swin_results = evaluate_model(swin_model, test_loader, device, \"Swin-Transformer\")\n",
    "        benchmark_results['Swin-Transformer'] = swin_results\n",
    "    else:\n",
    "        print(f\"Swin Transformer checkpoint not found at {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Swin Transformer model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "if benchmark_results:\n",
    "    comparison_data = []\n",
    "    for model_name, results in benchmark_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Overall Accuracy': f\"{results['overall_accuracy']:.4f}\",\n",
    "            'Mean Accuracy': f\"{results['mean_accuracy']:.4f}\",\n",
    "            'Mean IoU': f\"{results['mean_iou']:.4f}\",\n",
    "            'F1-Score (Micro)': f\"{results['f1_micro']:.4f}\",\n",
    "            'F1-Score (Macro)': f\"{results['f1_macro']:.4f}\",\n",
    "            'F1-Score (Weighted)': f\"{results['f1_weighted']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL BENCHMARK RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Overall Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    # Prepare data for plotting\n",
    "    models = list(benchmark_results.keys())\n",
    "    metrics = ['overall_accuracy', 'mean_accuracy', 'mean_iou', 'f1_weighted']\n",
    "    metric_names = ['Overall Accuracy', 'Mean Accuracy', 'Mean IoU', 'F1-Score (Weighted)']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "        values = [benchmark_results[model][metric] for model in models]\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        bars = ax.bar(models, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(models)])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{value:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        ax.set_title(metric_name, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontsize=10)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Per-Class Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    # Create per-class accuracy comparison\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(num_classes)\n",
    "    width = 0.25\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(benchmark_results.items()):\n",
    "        offset = (i - len(benchmark_results)/2 + 0.5) * width\n",
    "        bars = ax.bar(x + offset, results['per_class_accuracy'], width, \n",
    "                      label=model_name, color=colors[i % len(colors)])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, results['per_class_accuracy']):\n",
    "            if value > 0.05:  # Only show label if bar is visible\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                       f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([class_names[i] for i in range(num_classes)], rotation=45, ha='right')\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Per-Class IoU Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    # Create per-class IoU comparison\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(num_classes)\n",
    "    width = 0.25\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(benchmark_results.items()):\n",
    "        offset = (i - len(benchmark_results)/2 + 0.5) * width\n",
    "        bars = ax.bar(x + offset, results['per_class_iou'], width, \n",
    "                      label=model_name, color=colors[i % len(colors)], alpha=0.8)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, results['per_class_iou']):\n",
    "            if value > 0.05:  # Only show label if bar is visible\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                       f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('IoU', fontsize=12)\n",
    "    ax.set_title('Per-Class IoU Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([class_names[i] for i in range(num_classes)], rotation=45, ha='right')\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    # Plot confusion matrices\n",
    "    n_models = len(benchmark_results)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(8*n_models, 6))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(benchmark_results.items()):\n",
    "        cm = results['confusion_matrix']\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_normalized = np.nan_to_num(cm_normalized)  # Replace NaN with 0\n",
    "        \n",
    "        # Plot\n",
    "        im = axes[idx].imshow(cm_normalized, cmap='Blues', aspect='auto')\n",
    "        axes[idx].set_title(f'{model_name} Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
    "        axes[idx].set_ylabel('True Label', fontsize=10)\n",
    "        \n",
    "        # Set ticks\n",
    "        axes[idx].set_xticks(range(num_classes))\n",
    "        axes[idx].set_yticks(range(num_classes))\n",
    "        axes[idx].set_xticklabels([class_names[i][:4] for i in range(num_classes)], rotation=45)\n",
    "        axes[idx].set_yticklabels([class_names[i][:4] for i in range(num_classes)])\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                text = axes[idx].text(j, i, f'{cm_normalized[i, j]:.2f}',\n",
    "                                      ha=\"center\", va=\"center\", \n",
    "                                      color=\"white\" if cm_normalized[i, j] > 0.5 else \"black\",\n",
    "                                      fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Normalized Confusion Matrices', fontsize=14, fontweight='bold', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Radar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    from math import pi\n",
    "    \n",
    "    # Prepare data\n",
    "    categories = ['Overall\\nAccuracy', 'Mean\\nAccuracy', 'Mean\\nIoU', \n",
    "                  'F1-Score\\n(Micro)', 'F1-Score\\n(Macro)', 'F1-Score\\n(Weighted)']\n",
    "    metrics_keys = ['overall_accuracy', 'mean_accuracy', 'mean_iou', \n",
    "                    'f1_micro', 'f1_macro', 'f1_weighted']\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Compute angle for each axis\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(benchmark_results.items()):\n",
    "        values = [results[metric] for metric in metrics_keys]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[idx % len(colors)])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[idx % len(colors)])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=8)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "    plt.title('Multi-Metric Model Comparison', size=14, fontweight='bold', y=1.08)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    # Create detailed per-class comparison\n",
    "    class_comparison = pd.DataFrame()\n",
    "    \n",
    "    for model_name, results in benchmark_results.items():\n",
    "        model_data = {}\n",
    "        for i in range(num_classes):\n",
    "            model_data[f\"{class_names[i]}_Acc\"] = f\"{results['per_class_accuracy'][i]:.3f}\"\n",
    "            model_data[f\"{class_names[i]}_IoU\"] = f\"{results['per_class_iou'][i]:.3f}\"\n",
    "        \n",
    "        class_comparison = pd.concat([class_comparison, pd.DataFrame([model_data], index=[model_name])])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-CLASS DETAILED METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print accuracy columns\n",
    "    acc_cols = [col for col in class_comparison.columns if '_Acc' in col]\n",
    "    print(\"\\nAccuracy per Class:\")\n",
    "    print(class_comparison[acc_cols].to_string())\n",
    "    \n",
    "    # Print IoU columns\n",
    "    iou_cols = [col for col in class_comparison.columns if '_IoU' in col]\n",
    "    print(\"\\nIoU per Class:\")\n",
    "    print(class_comparison[iou_cols].to_string())\n",
    "    \n",
    "    # Best model determination\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST MODEL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    metrics_to_check = ['overall_accuracy', 'mean_iou', 'f1_weighted']\n",
    "    \n",
    "    for metric in metrics_to_check:\n",
    "        best_model = max(benchmark_results.items(), key=lambda x: x[1][metric])\n",
    "        print(f\"\\nBest {metric.replace('_', ' ').title()}: {best_model[0]} ({best_model[1][metric]:.4f})\")\n",
    "    \n",
    "    # Overall best model (average rank)\n",
    "    model_scores = {}\n",
    "    for model_name in benchmark_results.keys():\n",
    "        scores = [benchmark_results[model_name][metric] for metric in metrics_to_check]\n",
    "        model_scores[model_name] = np.mean(scores)\n",
    "    \n",
    "    best_overall = max(model_scores.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nüèÜ Overall Best Model: {best_overall[0]} (Average Score: {best_overall[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results to file\n",
    "if benchmark_results:\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Prepare results for saving\n",
    "    save_results = {}\n",
    "    for model_name, results in benchmark_results.items():\n",
    "        save_results[model_name] = {\n",
    "            'overall_accuracy': float(results['overall_accuracy']),\n",
    "            'mean_accuracy': float(results['mean_accuracy']),\n",
    "            'mean_iou': float(results['mean_iou']),\n",
    "            'f1_micro': float(results['f1_micro']),\n",
    "            'f1_macro': float(results['f1_macro']),\n",
    "            'f1_weighted': float(results['f1_weighted']),\n",
    "            'per_class_accuracy': [float(x) for x in results['per_class_accuracy']],\n",
    "            'per_class_iou': [float(x) for x in results['per_class_iou']]\n",
    "        }\n",
    "    \n",
    "    # Add metadata\n",
    "    save_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'test_set_size': len(test_loader.dataset),\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'results': save_results\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_path = './benchmark_results.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(save_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results saved to {output_path}\")\n",
    "    \n",
    "    # Also save comparison DataFrame to CSV\n",
    "    csv_path = './benchmark_comparison.csv'\n",
    "    comparison_df.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úÖ Comparison table saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This benchmark evaluation provides a comprehensive comparison of all trained models on the same test dataset. The analysis includes:\n",
    "\n",
    "1. **Overall Metrics**: Accuracy, IoU, and F1-scores\n",
    "2. **Per-Class Performance**: Detailed accuracy and IoU for each crop type\n",
    "3. **Visual Comparisons**: Bar charts, radar plots, and confusion matrices\n",
    "4. **Statistical Analysis**: Best model identification across different metrics\n",
    "\n",
    "The results are saved for future reference and can be used to select the best model for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}