{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a3f7a1",
   "metadata": {},
   "source": [
    "# TCN模型训练与评估\n",
    "\n",
    "本notebook实现了基于时序卷积网络(TCN)的农作物制图模型训练、评估和推理流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8a2a2",
   "metadata": {},
   "source": [
    "## 1. 环境设置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 导入自定义模块\n",
    "from TCN.model import create_tcn_model\n",
    "from TCN.dataset import prepare_data, save_data_info\n",
    "from TCN.utils import (\n",
    "    save_checkpoint, \n",
    "    calculate_metrics,\n",
    "    plot_training_history,\n",
    "    plot_confusion_matrix,\n",
    "    plot_class_performance\n",
    ")\n",
    "from TCN.train import FocalLoss\n",
    "\n",
    "# 设置matplotlib中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['PingFang SC', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6a3e3",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  data_path: ../dataset\n",
      "  patch_size: 32\n",
      "  stride: 16\n",
      "  test_size: 0.2\n",
      "  val_size: 0.1\n",
      "  batch_size: 8\n",
      "  num_workers: 2\n",
      "  input_channels: 8\n",
      "  temporal_steps: 28\n",
      "  tcn_channels: [64, 128, 256]\n",
      "  kernel_size: 3\n",
      "  dropout: 0.2\n",
      "  epochs: 50\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0.0001\n",
      "  use_focal_loss: True\n",
      "  save_dir: ./checkpoints\n",
      "  device: mps\n"
     ]
    }
   ],
   "source": [
    "# 训练配置\n",
    "\n",
    "# 首先，确定可用的最佳设备（GPU > MPS > CPU）\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "# 检查macOS的MPS后端是否可用\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "config = {\n",
    "    # 数据参数\n",
    "    'data_path': '../dataset',\n",
    "    'patch_size': 32,      # 从 64 降低到 32\n",
    "    'stride': 16,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.1,\n",
    "    'batch_size': 8,       # 从 16/8 降低到 4\n",
    "    'num_workers': 2,      # 从 4 降低到 2\n",
    "    \n",
    "    # 模型参数\n",
    "    'input_channels': 8,\n",
    "    'temporal_steps': 28,\n",
    "    'tcn_channels': [64, 128, 256],\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.2,\n",
    "    \n",
    "    # 训练参数\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'use_focal_loss': True,\n",
    "    \n",
    "    # 其他参数\n",
    "    'save_dir': './checkpoints',\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 创建保存目录\n",
    "Path(config['save_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3e3c6",
   "metadata": {},
   "source": [
    "## 3. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data loaders...\n",
      "Original data shape - X: (326, 1025, 28, 8), Y: (326, 1025)\n",
      "Dataset split - Train: 837, Val: 120, Test: 240\n",
      "Data info saved to ./checkpoints/data_info.pkl\n",
      "Number of classes: 9\n",
      "Class names: {0: 'Background', 1: 'Corn', 2: 'Wheat', 3: 'Sunflower', 4: 'Pumpkin', 5: 'Artificial_Surface', 6: 'Water', 7: 'Road', 8: 'Other'}\n",
      "Train samples: 837\n",
      "Validation samples: 120\n",
      "Test samples: 240\n",
      "Class weights: tensor([1.8564e+04, 2.0179e-01, 5.6892e+00, 6.0102e-01, 3.8582e+00, 4.9530e+00,\n",
      "        3.9785e+00, 1.5193e+00, 1.1988e+00])\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "print(\"Preparing data loaders...\")\n",
    "train_loader, val_loader, test_loader, data_info = prepare_data(\n",
    "    data_path=config['data_path'],\n",
    "    patch_size=config['patch_size'],\n",
    "    stride=config['stride'],\n",
    "    test_size=config['test_size'],\n",
    "    val_size=config['val_size'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers']\n",
    ")\n",
    "\n",
    "# 保存数据信息\n",
    "save_data_info(data_info, f\"{config['save_dir']}/data_info.pkl\")\n",
    "\n",
    "print(f\"Number of classes: {data_info['num_classes']}\")\n",
    "print(f\"Class names: {data_info['class_names']}\")\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "print(f\"Class weights: {data_info['class_weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3b3e1",
   "metadata": {},
   "source": [
    "## 4. 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model_creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model created successfully!\n",
      "Total parameters: 459,337\n",
      "Trainable parameters: 459,337\n",
      "Test input shape: torch.Size([2, 64, 64, 28, 8])\n",
      "Test output shape: torch.Size([2, 64, 64, 9])\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "device = torch.device(config['device'])\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = create_tcn_model(\n",
    "    input_channels=config['input_channels'],\n",
    "    temporal_steps=config['temporal_steps'],\n",
    "    num_classes=data_info['num_classes'],\n",
    "    tcn_channels=config['tcn_channels'],\n",
    "    kernel_size=config['kernel_size'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# 打印模型信息\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# 测试模型前向传播\n",
    "dummy_input = torch.randn(2, 64, 64, 28, 8).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"Test input shape: {dummy_input.shape}\")\n",
    "print(f\"Test output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5a4e5",
   "metadata": {},
   "source": [
    "## 5. 设置损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "setup_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Focal Loss\n",
      "Optimizer: AdamW (lr=0.001, weight_decay=0.0001)\n",
      "Scheduler: CosineAnnealingWarmRestarts\n"
     ]
    }
   ],
   "source": [
    "# 损失函数\n",
    "if config['use_focal_loss']:\n",
    "    criterion = FocalLoss(alpha=data_info['class_weights'].to(device), gamma=2.0)\n",
    "    print(\"Using Focal Loss\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(weight=data_info['class_weights'].to(device))\n",
    "    print(\"Using Cross Entropy Loss\")\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,\n",
    "    T_mult=2\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={config['learning_rate']}, weight_decay={config['weight_decay']})\")\n",
    "print(f\"Scheduler: CosineAnnealingWarmRestarts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8d4f1",
   "metadata": {},
   "source": [
    "## 6. 训练函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "training_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch, scaler):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} - Training')\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(pbar):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 混合精度\n",
    "        with autocast():\n",
    "            outputs = model(data)\n",
    "            outputs = outputs.permute(0, 3, 1, 2)  # (batch, num_classes, height, width)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 统计\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.numel()\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # 更新进度条\n",
    "        accuracy = 100. * correct / total\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Acc': f'{accuracy:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device, num_classes):\n",
    "    \"\"\"验证模型\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for data, targets in pbar:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # 混合精度验证\n",
    "            with autocast():\n",
    "                outputs = model(data)\n",
    "                outputs = outputs.permute(0, 3, 1, 2)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # 计算指标\n",
    "    all_preds = np.concatenate(all_preds).flatten()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "    \n",
    "    metrics = calculate_metrics(all_preds, all_targets, num_classes)\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    return val_loss, metrics['overall_accuracy'], metrics\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4b3e4",
   "metadata": {},
   "source": [
    "## 7. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_training",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc059a769e74438b669f5791ee70145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 - Training:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m train_loss, train_acc = train_epoch(\n\u001b[32m     27\u001b[39m     model, train_loader, criterion, optimizer, device, epoch, scaler\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[32m     31\u001b[39m val_loss, val_acc, val_metrics = validate(\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     model, val_loader, criterion, device, \u001b[43mdat\u001b[49m-info[\u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 学习率调度\u001b[39;00m\n\u001b[32m     36\u001b[39m scheduler.step()\n",
      "\u001b[31mNameError\u001b[39m: name 'dat' is not defined"
     ]
    }
   ],
   "source": [
    "# 训练历史记录\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_miou': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# 混合精度训练scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_miou = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{config['epochs']}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # 训练\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch, scaler\n",
    "    )\n",
    "    \n",
    "    # 验证\n",
    "    val_loss, val_acc, val_metrics = validate(\n",
    "        model, val_loader, criterion, device, dat_info['num_classes']\n",
    "    )\n",
    "    \n",
    "    # 学习率调度\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # 记录历史\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_miou'].append(val_metrics['mean_iou'])\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Val mIoU: {val_metrics['mean_iou']:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_metrics['mean_iou'] > best_miou:\n",
    "        best_miou = val_metrics['mean_iou']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_miou': best_miou,\n",
    "            'config': config,\n",
    "            'metrics': val_metrics\n",
    "        }, f\"{config['save_dir']}/best_model.pth\")\n",
    "        \n",
    "        print(f\"🎉 New best model saved! (mIoU: {best_miou:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # 早停检查\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n⏰ Early stopping triggered after {patience} epochs without improvement\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n✅ Training completed! Best mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4b3e5",
   "metadata": {},
   "source": [
    "## 8. 训练历史可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练历史\n",
    "plot_training_history(history, save_path=f\"{config['save_dir']}/training_curves.png\")\n",
    "\n",
    "# 保存训练历史\n",
    "with open(f\"{config['save_dir']}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(\"Training history plots generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3f2b6",
   "metadata": {},
   "source": [
    "## 9. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "checkpoint = torch.load(f\"{config['save_dir']}/best_model.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model (epoch {checkpoint['epoch']}, mIoU: {checkpoint['best_miou']:.4f})\")\n",
    "\n",
    "# 在测试集上评估\n",
    "test_loss, test_acc, test_metrics = validate(\n",
    "    model, test_loader, criterion, device, data_info['num_classes']\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Test mIoU: {test_metrics['mean_iou']:.4f}\")\n",
    "print(f\"Mean Class Accuracy: {test_metrics['mean_accuracy']:.4f}\")\n",
    "\n",
    "# 详细的每类指标\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "for i, class_name in data_info['class_names'].items():\n",
    "    if i < len(test_metrics['per_class_accuracy']):\n",
    "        acc = test_metrics['per_class_accuracy'][i]\n",
    "        iou = test_metrics['per_class_iou'][i]\n",
    "        print(f\"  {class_name}: Acc={acc:.3f}, IoU={iou:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3b5e6",
   "metadata": {},
   "source": [
    "## 10. 评估结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "class_names_list = [data_info['class_names'][i] for i in range(data_info['num_classes'])]\n",
    "plot_confusion_matrix(\n",
    "    test_metrics['confusion_matrix'],\n",
    "    class_names_list,\n",
    "    save_path=f\"{config['save_dir']}/confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "# 绘制类别性能图\n",
    "plot_class_performance(\n",
    "    test_metrics, \n",
    "    save_path=f\"{config['save_dir']}/class_performance.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e739",
   "metadata": {},
   "source": [
    "## 11. 预测样本可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCN.utils import visualize_predictions\n",
    "\n",
    "# 获取一批测试数据\n",
    "data_iter = iter(test_loader)\n",
    "test_batch_data, test_batch_targets = next(data_iter)\n",
    "\n",
    "# 预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_data_device = test_batch_data.to(device)\n",
    "    test_outputs = model(test_batch_data_device)\n",
    "    _, test_predictions = test_outputs.max(-1)\n",
    "\n",
    "# 可视化前5个样本的预测结果\n",
    "visualize_predictions(\n",
    "    test_batch_data[:5],\n",
    "    test_batch_targets[:5],\n",
    "    test_predictions[:5].cpu(),\n",
    "    class_names_list,\n",
    "    num_samples=5,\n",
    "    save_path=f\"{config['save_dir']}/predictions_visualization.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3f5c6",
   "metadata": {},
   "source": [
    "## 12. 模型推理示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCN.inference import CropMappingInference\n",
    "\n",
    "# 初始化推理器\n",
    "inferencer = CropMappingInference(\n",
    "    model_path=f\"{config['save_dir']}/best_model.pth\",\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "# 加载完整数据集进行推理演示（使用较小的区域）\n",
    "full_x_data = np.load('../dataset/x.npy')\n",
    "full_y_data = np.load('../dataset/y.npy')\n",
    "\n",
    "# 选择一个较小的区域进行快速推理演示\n",
    "crop_size = 128\n",
    "x_crop = full_x_data[:crop_size, :crop_size]\n",
    "y_crop = full_y_data[:crop_size, :crop_size]\n",
    "\n",
    "print(f\"Running inference on {x_crop.shape[:2]} region...\")\n",
    "\n",
    "# 进行推理\n",
    "predictions = inferencer.predict_full_image(\n",
    "    x_crop,\n",
    "    patch_size=32,  # 使用较小的patch以加快推理\n",
    "    overlap=8,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# 可视化推理结果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 显示RGB合成图（使用第一个时间步的前三个波段）\n",
    "rgb = x_crop[:, :, 0, :3]\n",
    "rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "axes[0].imshow(rgb)\n",
    "axes[0].set_title('Input (RGB Composite)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 显示真实标签\n",
    "im1 = axes[1].imshow(y_crop, cmap='tab10', vmin=0, vmax=8)\n",
    "axes[1].set_title('Ground Truth')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# 显示预测结果\n",
    "axes[2].imshow(predictions, cmap='tab10', vmin=0, vmax=8)\n",
    "axes[2].set_title('TCN Predictions')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(im1, ax=axes, orientation='horizontal', fraction=0.05, pad=0.1)\n",
    "cbar.set_ticks(range(len(class_names_list)))\n",
    "cbar.set_ticklabels([name.split('(')[0].strip() if '(' in name else name for name in class_names_list])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['save_dir']}/inference_example.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 计算推理准确率\n",
    "inference_acc = np.mean(predictions == y_crop)\n",
    "print(f\"\\nInference accuracy on cropped region: {inference_acc:.4f}\")\n",
    "\n",
    "# 统计预测分布\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "print(\"\\nPrediction distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    percentage = count / predictions.size * 100\n",
    "    class_name = data_info['class_names'].get(label, f\"Unknown({label})\")\n",
    "    print(f\"  {class_name}: {count:,} pixels ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7a8d3",
   "metadata": {},
   "source": [
    "## 13. 总结与结论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成最终总结报告\n",
    "summary_report = f\"\"\"\n",
    "=== TCN模型训练总结报告 ===\n",
    "训练时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "设备: {device}\n",
    "\n",
    "模型配置:\n",
    "- 输入通道数: {config['input_channels']}\n",
    "- 时间步数: {config['temporal_steps']}\n",
    "- TCN通道: {config['tcn_channels']}\n",
    "- 类别数: {data_info['num_classes']}\n",
    "- 总参数量: {total_params:,}\n",
    "\n",
    "训练配置:\n",
    "- 训练轮数: {len(history['train_loss'])}\n",
    "- 批次大小: {config['batch_size']}\n",
    "- 学习率: {config['learning_rate']}\n",
    "- 切片大小: {config['patch_size']}\n",
    "\n",
    "最佳性能 (验证集):\n",
    "- mIoU: {best_miou:.4f}\n",
    "- 准确率: {max(history['val_acc']):.2f}%\n",
    "\n",
    "测试集性能:\n",
    "- mIoU: {test_metrics['mean_iou']:.4f}\n",
    "- 总体准确率: {test_metrics['overall_accuracy']:.4f}\n",
    "- 平均类别准确率: {test_metrics['mean_accuracy']:.4f}\n",
    "\n",
    "文件输出:\n",
    "- 最佳模型: {config['save_dir']}/best_model.pth\n",
    "- 训练历史: {config['save_dir']}/history.json\n",
    "- 数据信息: {config['save_dir']}/data_info.pkl\n",
    "- 训练曲线: {config['save_dir']}/training_curves.png\n",
    "- 混淆矩阵: {config['save_dir']}/confusion_matrix.png\n",
    "\n",
    "=== 报告结束 ===\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# 保存报告\n",
    "with open(f\"{config['save_dir']}/training_summary.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\n🎯 训练完成！所有结果已保存到: {config['save_dir']}/\")\n",
    "print(f\"📊 最佳模型mIoU: {best_miou:.4f}\")\n",
    "print(f\"📈 测试集mIoU: {test_metrics['mean_iou']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepCropMapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
