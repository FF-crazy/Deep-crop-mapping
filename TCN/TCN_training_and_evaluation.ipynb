{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a3f7a1",
   "metadata": {},
   "source": [
    "# TCNæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "\n",
    "æœ¬notebookå®ç°äº†åŸºäºæ—¶åºå·ç§¯ç½‘ç»œ(TCN)çš„å†œä½œç‰©åˆ¶å›¾æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œæ¨ç†æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8a2a2",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®ä¸å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—\n",
    "from TCN.model import create_tcn_model\n",
    "from TCN.dataset import prepare_data, save_data_info\n",
    "from TCN.utils import (\n",
    "    save_checkpoint, \n",
    "    calculate_metrics,\n",
    "    plot_training_history,\n",
    "    plot_confusion_matrix,\n",
    "    plot_class_performance\n",
    ")\n",
    "from TCN.train import FocalLoss\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['PingFang SC', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6a3e3",
   "metadata": {},
   "source": [
    "## 2. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  data_path: ../dataset\n",
      "  patch_size: 32\n",
      "  stride: 16\n",
      "  test_size: 0.2\n",
      "  val_size: 0.1\n",
      "  batch_size: 8\n",
      "  num_workers: 2\n",
      "  input_channels: 8\n",
      "  temporal_steps: 28\n",
      "  tcn_channels: [64, 128, 256]\n",
      "  kernel_size: 3\n",
      "  dropout: 0.2\n",
      "  epochs: 50\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0.0001\n",
      "  use_focal_loss: True\n",
      "  save_dir: ./checkpoints\n",
      "  device: mps\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒé…ç½®\n",
    "\n",
    "# é¦–å…ˆï¼Œç¡®å®šå¯ç”¨çš„æœ€ä½³è®¾å¤‡ï¼ˆGPU > MPS > CPUï¼‰\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "# æ£€æŸ¥macOSçš„MPSåç«¯æ˜¯å¦å¯ç”¨\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "config = {\n",
    "    # æ•°æ®å‚æ•°\n",
    "    'data_path': '../dataset',\n",
    "    'patch_size': 32,      # ä» 64 é™ä½åˆ° 32\n",
    "    'stride': 16,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.1,\n",
    "    'batch_size': 8,       # ä» 16/8 é™ä½åˆ° 4\n",
    "    'num_workers': 2,      # ä» 4 é™ä½åˆ° 2\n",
    "    \n",
    "    # æ¨¡å‹å‚æ•°\n",
    "    'input_channels': 8,\n",
    "    'temporal_steps': 28,\n",
    "    'tcn_channels': [64, 128, 256],\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.2,\n",
    "    \n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'use_focal_loss': True,\n",
    "    \n",
    "    # å…¶ä»–å‚æ•°\n",
    "    'save_dir': './checkpoints',\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# åˆ›å»ºä¿å­˜ç›®å½•\n",
    "Path(config['save_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3e3c6",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data loaders...\n",
      "Original data shape - X: (326, 1025, 28, 8), Y: (326, 1025)\n",
      "Dataset split - Train: 837, Val: 120, Test: 240\n",
      "Data info saved to ./checkpoints/data_info.pkl\n",
      "Number of classes: 9\n",
      "Class names: {0: 'Background', 1: 'Corn', 2: 'Wheat', 3: 'Sunflower', 4: 'Pumpkin', 5: 'Artificial_Surface', 6: 'Water', 7: 'Road', 8: 'Other'}\n",
      "Train samples: 837\n",
      "Validation samples: 120\n",
      "Test samples: 240\n",
      "Class weights: tensor([1.8564e+04, 2.0179e-01, 5.6892e+00, 6.0102e-01, 3.8582e+00, 4.9530e+00,\n",
      "        3.9785e+00, 1.5193e+00, 1.1988e+00])\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡æ•°æ®\n",
    "print(\"Preparing data loaders...\")\n",
    "train_loader, val_loader, test_loader, data_info = prepare_data(\n",
    "    data_path=config['data_path'],\n",
    "    patch_size=config['patch_size'],\n",
    "    stride=config['stride'],\n",
    "    test_size=config['test_size'],\n",
    "    val_size=config['val_size'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers']\n",
    ")\n",
    "\n",
    "# ä¿å­˜æ•°æ®ä¿¡æ¯\n",
    "save_data_info(data_info, f\"{config['save_dir']}/data_info.pkl\")\n",
    "\n",
    "print(f\"Number of classes: {data_info['num_classes']}\")\n",
    "print(f\"Class names: {data_info['class_names']}\")\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "print(f\"Class weights: {data_info['class_weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3b3e1",
   "metadata": {},
   "source": [
    "## 4. åˆ›å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model_creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model created successfully!\n",
      "Total parameters: 459,337\n",
      "Trainable parameters: 459,337\n",
      "Test input shape: torch.Size([2, 64, 64, 28, 8])\n",
      "Test output shape: torch.Size([2, 64, 64, 9])\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºæ¨¡å‹\n",
    "device = torch.device(config['device'])\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = create_tcn_model(\n",
    "    input_channels=config['input_channels'],\n",
    "    temporal_steps=config['temporal_steps'],\n",
    "    num_classes=data_info['num_classes'],\n",
    "    tcn_channels=config['tcn_channels'],\n",
    "    kernel_size=config['kernel_size'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# æ‰“å°æ¨¡å‹ä¿¡æ¯\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­\n",
    "dummy_input = torch.randn(2, 64, 64, 28, 8).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"Test input shape: {dummy_input.shape}\")\n",
    "print(f\"Test output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5a4e5",
   "metadata": {},
   "source": [
    "## 5. è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "setup_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Focal Loss\n",
      "Optimizer: AdamW (lr=0.001, weight_decay=0.0001)\n",
      "Scheduler: CosineAnnealingWarmRestarts\n"
     ]
    }
   ],
   "source": [
    "# æŸå¤±å‡½æ•°\n",
    "if config['use_focal_loss']:\n",
    "    criterion = FocalLoss(alpha=data_info['class_weights'].to(device), gamma=2.0)\n",
    "    print(\"Using Focal Loss\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(weight=data_info['class_weights'].to(device))\n",
    "    print(\"Using Cross Entropy Loss\")\n",
    "\n",
    "# ä¼˜åŒ–å™¨\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,\n",
    "    T_mult=2\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={config['learning_rate']}, weight_decay={config['weight_decay']})\")\n",
    "print(f\"Scheduler: CosineAnnealingWarmRestarts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8d4f1",
   "metadata": {},
   "source": [
    "## 6. è®­ç»ƒå‡½æ•°å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "training_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch, scaler):\n",
    "    \"\"\"è®­ç»ƒä¸€ä¸ªepoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} - Training')\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(pbar):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # æ··åˆç²¾åº¦\n",
    "        with autocast():\n",
    "            outputs = model(data)\n",
    "            outputs = outputs.permute(0, 3, 1, 2)  # (batch, num_classes, height, width)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.numel()\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # æ›´æ–°è¿›åº¦æ¡\n",
    "        accuracy = 100. * correct / total\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Acc': f'{accuracy:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device, num_classes):\n",
    "    \"\"\"éªŒè¯æ¨¡å‹\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for data, targets in pbar:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # æ··åˆç²¾åº¦éªŒè¯\n",
    "            with autocast():\n",
    "                outputs = model(data)\n",
    "                outputs = outputs.permute(0, 3, 1, 2)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    all_preds = np.concatenate(all_preds).flatten()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "    \n",
    "    metrics = calculate_metrics(all_preds, all_targets, num_classes)\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    return val_loss, metrics['overall_accuracy'], metrics\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4b3e4",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_training",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc059a769e74438b669f5791ee70145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 - Training:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m train_loss, train_acc = train_epoch(\n\u001b[32m     27\u001b[39m     model, train_loader, criterion, optimizer, device, epoch, scaler\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# éªŒè¯\u001b[39;00m\n\u001b[32m     31\u001b[39m val_loss, val_acc, val_metrics = validate(\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     model, val_loader, criterion, device, \u001b[43mdat\u001b[49m-info[\u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# å­¦ä¹ ç‡è°ƒåº¦\u001b[39;00m\n\u001b[32m     36\u001b[39m scheduler.step()\n",
      "\u001b[31mNameError\u001b[39m: name 'dat' is not defined"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒå†å²è®°å½•\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_miou': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# æ··åˆç²¾åº¦è®­ç»ƒscaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_miou = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{config['epochs']}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch, scaler\n",
    "    )\n",
    "    \n",
    "    # éªŒè¯\n",
    "    val_loss, val_acc, val_metrics = validate(\n",
    "        model, val_loader, criterion, device, dat_info['num_classes']\n",
    "    )\n",
    "    \n",
    "    # å­¦ä¹ ç‡è°ƒåº¦\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # è®°å½•å†å²\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_miou'].append(val_metrics['mean_iou'])\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Val mIoU: {val_metrics['mean_iou']:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    if val_metrics['mean_iou'] > best_miou:\n",
    "        best_miou = val_metrics['mean_iou']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_miou': best_miou,\n",
    "            'config': config,\n",
    "            'metrics': val_metrics\n",
    "        }, f\"{config['save_dir']}/best_model.pth\")\n",
    "        \n",
    "        print(f\"ğŸ‰ New best model saved! (mIoU: {best_miou:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # æ—©åœæ£€æŸ¥\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nâ° Early stopping triggered after {patience} epochs without improvement\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… Training completed! Best mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4b3e5",
   "metadata": {},
   "source": [
    "## 8. è®­ç»ƒå†å²å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "plot_training_history(history, save_path=f\"{config['save_dir']}/training_curves.png\")\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒå†å²\n",
    "with open(f\"{config['save_dir']}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(\"Training history plots generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3f2b6",
   "metadata": {},
   "source": [
    "## 9. æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "checkpoint = torch.load(f\"{config['save_dir']}/best_model.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model (epoch {checkpoint['epoch']}, mIoU: {checkpoint['best_miou']:.4f})\")\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "test_loss, test_acc, test_metrics = validate(\n",
    "    model, test_loader, criterion, device, data_info['num_classes']\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Test mIoU: {test_metrics['mean_iou']:.4f}\")\n",
    "print(f\"Mean Class Accuracy: {test_metrics['mean_accuracy']:.4f}\")\n",
    "\n",
    "# è¯¦ç»†çš„æ¯ç±»æŒ‡æ ‡\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "for i, class_name in data_info['class_names'].items():\n",
    "    if i < len(test_metrics['per_class_accuracy']):\n",
    "        acc = test_metrics['per_class_accuracy'][i]\n",
    "        iou = test_metrics['per_class_iou'][i]\n",
    "        print(f\"  {class_name}: Acc={acc:.3f}, IoU={iou:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3b5e6",
   "metadata": {},
   "source": [
    "## 10. è¯„ä¼°ç»“æœå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "class_names_list = [data_info['class_names'][i] for i in range(data_info['num_classes'])]\n",
    "plot_confusion_matrix(\n",
    "    test_metrics['confusion_matrix'],\n",
    "    class_names_list,\n",
    "    save_path=f\"{config['save_dir']}/confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "# ç»˜åˆ¶ç±»åˆ«æ€§èƒ½å›¾\n",
    "plot_class_performance(\n",
    "    test_metrics, \n",
    "    save_path=f\"{config['save_dir']}/class_performance.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e739",
   "metadata": {},
   "source": [
    "## 11. é¢„æµ‹æ ·æœ¬å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCN.utils import visualize_predictions\n",
    "\n",
    "# è·å–ä¸€æ‰¹æµ‹è¯•æ•°æ®\n",
    "data_iter = iter(test_loader)\n",
    "test_batch_data, test_batch_targets = next(data_iter)\n",
    "\n",
    "# é¢„æµ‹\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_data_device = test_batch_data.to(device)\n",
    "    test_outputs = model(test_batch_data_device)\n",
    "    _, test_predictions = test_outputs.max(-1)\n",
    "\n",
    "# å¯è§†åŒ–å‰5ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ\n",
    "visualize_predictions(\n",
    "    test_batch_data[:5],\n",
    "    test_batch_targets[:5],\n",
    "    test_predictions[:5].cpu(),\n",
    "    class_names_list,\n",
    "    num_samples=5,\n",
    "    save_path=f\"{config['save_dir']}/predictions_visualization.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3f5c6",
   "metadata": {},
   "source": [
    "## 12. æ¨¡å‹æ¨ç†ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCN.inference import CropMappingInference\n",
    "\n",
    "# åˆå§‹åŒ–æ¨ç†å™¨\n",
    "inferencer = CropMappingInference(\n",
    "    model_path=f\"{config['save_dir']}/best_model.pth\",\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "# åŠ è½½å®Œæ•´æ•°æ®é›†è¿›è¡Œæ¨ç†æ¼”ç¤ºï¼ˆä½¿ç”¨è¾ƒå°çš„åŒºåŸŸï¼‰\n",
    "full_x_data = np.load('../dataset/x.npy')\n",
    "full_y_data = np.load('../dataset/y.npy')\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„åŒºåŸŸè¿›è¡Œå¿«é€Ÿæ¨ç†æ¼”ç¤º\n",
    "crop_size = 128\n",
    "x_crop = full_x_data[:crop_size, :crop_size]\n",
    "y_crop = full_y_data[:crop_size, :crop_size]\n",
    "\n",
    "print(f\"Running inference on {x_crop.shape[:2]} region...\")\n",
    "\n",
    "# è¿›è¡Œæ¨ç†\n",
    "predictions = inferencer.predict_full_image(\n",
    "    x_crop,\n",
    "    patch_size=32,  # ä½¿ç”¨è¾ƒå°çš„patchä»¥åŠ å¿«æ¨ç†\n",
    "    overlap=8,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# å¯è§†åŒ–æ¨ç†ç»“æœ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# æ˜¾ç¤ºRGBåˆæˆå›¾ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„å‰ä¸‰ä¸ªæ³¢æ®µï¼‰\n",
    "rgb = x_crop[:, :, 0, :3]\n",
    "rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "axes[0].imshow(rgb)\n",
    "axes[0].set_title('Input (RGB Composite)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# æ˜¾ç¤ºçœŸå®æ ‡ç­¾\n",
    "im1 = axes[1].imshow(y_crop, cmap='tab10', vmin=0, vmax=8)\n",
    "axes[1].set_title('Ground Truth')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# æ˜¾ç¤ºé¢„æµ‹ç»“æœ\n",
    "axes[2].imshow(predictions, cmap='tab10', vmin=0, vmax=8)\n",
    "axes[2].set_title('TCN Predictions')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# æ·»åŠ é¢œè‰²æ¡\n",
    "cbar = plt.colorbar(im1, ax=axes, orientation='horizontal', fraction=0.05, pad=0.1)\n",
    "cbar.set_ticks(range(len(class_names_list)))\n",
    "cbar.set_ticklabels([name.split('(')[0].strip() if '(' in name else name for name in class_names_list])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['save_dir']}/inference_example.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# è®¡ç®—æ¨ç†å‡†ç¡®ç‡\n",
    "inference_acc = np.mean(predictions == y_crop)\n",
    "print(f\"\\nInference accuracy on cropped region: {inference_acc:.4f}\")\n",
    "\n",
    "# ç»Ÿè®¡é¢„æµ‹åˆ†å¸ƒ\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "print(\"\\nPrediction distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    percentage = count / predictions.size * 100\n",
    "    class_name = data_info['class_names'].get(label, f\"Unknown({label})\")\n",
    "    print(f\"  {class_name}: {count:,} pixels ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7a8d3",
   "metadata": {},
   "source": [
    "## 13. æ€»ç»“ä¸ç»“è®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š\n",
    "summary_report = f\"\"\"\n",
    "=== TCNæ¨¡å‹è®­ç»ƒæ€»ç»“æŠ¥å‘Š ===\n",
    "è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "è®¾å¤‡: {device}\n",
    "\n",
    "æ¨¡å‹é…ç½®:\n",
    "- è¾“å…¥é€šé“æ•°: {config['input_channels']}\n",
    "- æ—¶é—´æ­¥æ•°: {config['temporal_steps']}\n",
    "- TCNé€šé“: {config['tcn_channels']}\n",
    "- ç±»åˆ«æ•°: {data_info['num_classes']}\n",
    "- æ€»å‚æ•°é‡: {total_params:,}\n",
    "\n",
    "è®­ç»ƒé…ç½®:\n",
    "- è®­ç»ƒè½®æ•°: {len(history['train_loss'])}\n",
    "- æ‰¹æ¬¡å¤§å°: {config['batch_size']}\n",
    "- å­¦ä¹ ç‡: {config['learning_rate']}\n",
    "- åˆ‡ç‰‡å¤§å°: {config['patch_size']}\n",
    "\n",
    "æœ€ä½³æ€§èƒ½ (éªŒè¯é›†):\n",
    "- mIoU: {best_miou:.4f}\n",
    "- å‡†ç¡®ç‡: {max(history['val_acc']):.2f}%\n",
    "\n",
    "æµ‹è¯•é›†æ€§èƒ½:\n",
    "- mIoU: {test_metrics['mean_iou']:.4f}\n",
    "- æ€»ä½“å‡†ç¡®ç‡: {test_metrics['overall_accuracy']:.4f}\n",
    "- å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {test_metrics['mean_accuracy']:.4f}\n",
    "\n",
    "æ–‡ä»¶è¾“å‡º:\n",
    "- æœ€ä½³æ¨¡å‹: {config['save_dir']}/best_model.pth\n",
    "- è®­ç»ƒå†å²: {config['save_dir']}/history.json\n",
    "- æ•°æ®ä¿¡æ¯: {config['save_dir']}/data_info.pkl\n",
    "- è®­ç»ƒæ›²çº¿: {config['save_dir']}/training_curves.png\n",
    "- æ··æ·†çŸ©é˜µ: {config['save_dir']}/confusion_matrix.png\n",
    "\n",
    "=== æŠ¥å‘Šç»“æŸ ===\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# ä¿å­˜æŠ¥å‘Š\n",
    "with open(f\"{config['save_dir']}/training_summary.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nğŸ¯ è®­ç»ƒå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {config['save_dir']}/\")\n",
    "print(f\"ğŸ“Š æœ€ä½³æ¨¡å‹mIoU: {best_miou:.4f}\")\n",
    "print(f\"ğŸ“ˆ æµ‹è¯•é›†mIoU: {test_metrics['mean_iou']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepCropMapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
