#!/usr/bin/env python3
"""
æµ‹è¯•Transformeræ¨¡å‹å®ç°çš„è„šæœ¬
"""

def test_transformer_import():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    try:
        # æµ‹è¯•æ‰€æœ‰å¿…è¦çš„å¯¼å…¥
        import torch
        print("âœ“ PyTorchå¯¼å…¥æˆåŠŸ")
        
        import torch.nn as nn
        print("âœ“ torch.nnå¯¼å…¥æˆåŠŸ")
        
        from Transformer.model import create_transformer_model, CropMappingTransformer
        print("âœ“ Transformeræ¨¡å‹å¯¼å…¥æˆåŠŸ")
        
        from Transformer.dataset import prepare_data, CropMappingDataset
        print("âœ“ Transformeræ•°æ®é›†å¯¼å…¥æˆåŠŸ")
        
        from Transformer.utils import calculate_metrics, plot_training_history
        print("âœ“ Transformerå·¥å…·å‡½æ•°å¯¼å…¥æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    try:
        import torch
        from Transformer.model import create_transformer_model
        
        print("\nå¼€å§‹æµ‹è¯•Transformeræ¨¡å‹åˆ›å»º...")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_transformer_model(
            input_channels=8,
            temporal_steps=28,
            num_classes=8,
            patch_size=8,
            embed_dim=256,
            num_layers=6,
            num_heads=8
        )
        
        print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ“ æ€»å‚æ•°é‡: {total_params:,}")
        print(f"âœ“ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None


def test_forward_pass(model):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    try:
        import torch
        
        print("\nå¼€å§‹æµ‹è¯•å‰å‘ä¼ æ’­...")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        height, width = 64, 64
        temporal_steps = 28
        input_channels = 8
        
        dummy_input = torch.randn(batch_size, height, width, temporal_steps, input_channels)
        print(f"âœ“ æµ‹è¯•è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
        
        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            output = model(dummy_input)
        
        expected_shape = (batch_size, height, width, 8)  # 8ä¸ªç±»åˆ«
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"âœ“ æœŸæœ›å½¢çŠ¶: {expected_shape}")
        
        if output.shape == expected_shape:
            print("âœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ!")
            return True
        else:
            print("âœ— è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…")
            return False
            
    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_loss_functions():
    """æµ‹è¯•æŸå¤±å‡½æ•°"""
    try:
        import torch
        import torch.nn as nn
        from Transformer.train import FocalLoss, DiceLoss, CombinedLoss
        
        print("\nå¼€å§‹æµ‹è¯•æŸå¤±å‡½æ•°...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        num_classes = 8
        height, width = 32, 32
        
        # æ¨¡æ‹Ÿè¾“å‡º (batch, num_classes, height, width)
        outputs = torch.randn(batch_size, num_classes, height, width)
        targets = torch.randint(0, num_classes, (batch_size, height, width))
        
        # æµ‹è¯•Focal Loss
        focal_loss = FocalLoss(gamma=2.5)
        focal_result = focal_loss(outputs, targets)
        print(f"âœ“ Focal Loss: {focal_result.item():.4f}")
        
        # æµ‹è¯•Dice Loss
        dice_loss = DiceLoss()
        dice_result = dice_loss(outputs, targets)
        print(f"âœ“ Dice Loss: {dice_result.item():.4f}")
        
        # æµ‹è¯•ç»„åˆæŸå¤±
        combined_loss = CombinedLoss()
        combined_result = combined_loss(outputs, targets)
        print(f"âœ“ Combined Loss: {combined_result.item():.4f}")
        
        print("âœ“ æŸå¤±å‡½æ•°æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âœ— æŸå¤±å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("TRANSFORMERæ¨¡å‹å®ç°æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1: æ¨¡å—å¯¼å…¥
    print("\n1. æµ‹è¯•æ¨¡å—å¯¼å…¥")
    if not test_transformer_import():
        print("æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
        return
    
    # æµ‹è¯•2: æ¨¡å‹åˆ›å»º
    print("\n2. æµ‹è¯•æ¨¡å‹åˆ›å»º")
    model = test_model_creation()
    if model is None:
        print("æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
        return
    
    # æµ‹è¯•3: å‰å‘ä¼ æ’­
    print("\n3. æµ‹è¯•å‰å‘ä¼ æ’­")
    if not test_forward_pass(model):
        print("å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•4: æŸå¤±å‡½æ•°
    print("\n4. æµ‹è¯•æŸå¤±å‡½æ•°")
    if not test_loss_functions():
        print("æŸå¤±å‡½æ•°æµ‹è¯•å¤±è´¥")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Transformerå®ç°æ­£å¸¸å·¥ä½œ!")
    print("=" * 60)
    
    # è¾“å‡ºä½¿ç”¨å»ºè®®
    print("\nğŸ“‹ ä½¿ç”¨å»ºè®®:")
    print("1. è®­ç»ƒæ¨¡å‹:")
    print("   python -m Transformer.train --data-path ./dataset --epochs 50")
    
    print("\n2. è¯„ä¼°æ¨¡å‹:")
    print("   python -m Transformer.evaluate --model-path ./Transformer/checkpoints/best_model.pth")
    
    print("\n3. æ¨ç†é¢„æµ‹:")
    print("   python -m Transformer.inference --model-path ./Transformer/checkpoints/best_model.pth --input-path ./dataset/x.npy")


if __name__ == "__main__":
    main()